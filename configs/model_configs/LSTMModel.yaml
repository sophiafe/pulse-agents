name: "LSTMModel"
pretrained_model_paths: null # "/cluster/project/bmds_lab/sepsis_sophia/pulse/output/output_to_keep/20250525_185834_LSTM/Models" # Path to pretrained model (if any)
params:
  # Basic configuration
  trainer_name: "LSTMTrainer"
  type: "convDL" # Type of model (convML for conventional machine learning, convDL for conventional deep learning and LLM for large language models)
  architecture_type: "RNN" # Architecture type (CNN, RNN)
  mode: "train"
  save_checkpoint: 0 # Save checkpoint every n epochs. Set to 0 to disable.
  verbose: 2 # Controls verbosity of output (0 = silent, 1 = log every 100 batches, 2 = log every batch)

  # Model Architecture
  hidden_size: 64 # Number of hidden units in LSTM layers
  num_layers: 3 # Number of LSTM layers (should match length of lstm_units)
  lstm_units: [256, 128, 64] # Number of units in each LSTM layer
  dense_units: 64 # Number of units in the first dense layer
  dropout: [0.2, 0.3, 0.4] # Dropout rate for LSTM layers, can be single value or list
  output_shape: 1

  # Training
  num_epochs: 100
  early_stopping_rounds: 10

  # Optimization
  learning_rate: 0.001
  grad_clip_max_norm: 1.0 # max_norm = 1.0 (LSTM, GRU) or 2.0 (CNN, InceptionTime)

  # Scheduler
  scheduler_factor: 0.1
  scheduler_patience: 5
  scheduler_cooldown: 0
  min_lr: 0.000001 # 1e-6
