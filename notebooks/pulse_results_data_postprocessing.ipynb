{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2361f3a8",
   "metadata": {},
   "source": [
    "# Metadata Postprocessing & Metrics Calculation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37960cfd",
   "metadata": {},
   "source": [
    "## Imports and Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d299e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from sklearn.calibration import expit\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "# Set sys.path to the parent directory of the current working directory\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "from src.eval.metrics import calculate_all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "456291f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global task and dataset order for consistent plotting\n",
    "TASK_MAPPING = OrderedDict([\n",
    "    (\"mortality\", \"Mortality\"),\n",
    "    (\"aki\", \"AKI\"),\n",
    "    (\"sepsis\", \"Sepsis\"),\n",
    "])\n",
    "\n",
    "DATASET_MAPPING = OrderedDict([\n",
    "    (\"hirid\", \"HiRID\"),\n",
    "    (\"miiv\", \"MIMIC-IV\"),\n",
    "    (\"eicu\", \"eICU\"),\n",
    "])\n",
    "MODEL_LIST = [\n",
    "    \"RandomForest\", \"LightGBM\", \"XGBoost\", \"CNN\", \"InceptionTime\", \"LSTM\", \"GRU\",\n",
    "    \"GPT4o\", \"Gemini2p5flash\", \"Llama3\", \"Mistral\", \"DeepseekR1Llama8b\", \"Gemma3\", \"MedGemma\"\n",
    "]\n",
    "\n",
    "CONVML_MODELS = [\"RandomForest\", \"XGBoost\", \"LightGBM\"]\n",
    "CONVDL_MODELS = [\"CNN\", \"InceptionTime\", \"GRU\", \"LSTM\"]\n",
    "LLM_MODELS = {\n",
    "    \"OpenAIo3\": \"OpenAI-o3\",\n",
    "    \"ClaudeSonnet4\": \"Claude-Sonnet-4\",\n",
    "    \"Grok4\": \"Grok-4\",\n",
    "    \"Gemini2p5pro\": \"Gemini-2.5-Pro\",\n",
    "    \"Gemini2p5flash\": \"Gemini-2.5-Flash\",\n",
    "    \"Llama3\": \"Llama-3.1-8B-Instruct\",\n",
    "    \"DeepseekR1Llama8b\": \"Deepseek-R1-Distill-Llama-8B\",\n",
    "    \"Mistral\": \"Mistral-7B-Instruct-v0.3\",\n",
    "    \"Gemma3\": \"Gemma-3-4B-it\",  # 4B or 12B\n",
    "    \"MedGemma\": \"MedGemma-4B-it\",\n",
    "}\n",
    "\n",
    "SUBGROUP_TYPES = [\"Sex\", \"Age\", \"BMI\"]\n",
    "SEX_LIST = [\"Male\", \"Female\"]\n",
    "AGE_LIST = [\"18-65 Years\", \"65-75 Years\", \"75-91 Years\"]\n",
    "BMI_LIST = [\"BMI < 18.5 kg/m2\", \"BMI 18.5-25 kg/m2\", \"BMI 25-30 kg/m2\", \"BMI > 30 kg/m2\"]\n",
    "\n",
    "PROMPTING_ID_DISPLAY_MAPPING = {\n",
    "    \"sarvari_2024_aggregation_preprocessor\": \"Aggregation\",\n",
    "    \"zhu_2024b_zero_shot_preprocessor\": \"Zero-Shot\",\n",
    "    \"zhu_2024b_one_shot_preprocessor\": \"One-Shot\",\n",
    "    \"liu_2023_few_shot_preprocessor\": \"Few-Shot (3)\",\n",
    "    \"zhu_2024a_cot_preprocessor\": \"CoT\",\n",
    "    \"zhu_2024c_categorization_summary_agent_preprocessor\": \"SumAgent\",\n",
    "    \"collaborative_reasoning_agent_preprocessor\": \"ColAgent\",\n",
    "    \"clinical_workflow_agent_preprocessor\": \"ClinFlowAgent\",\n",
    "    \"hybrid_reasoning_agent_preprocessor\": \"HybReAgent\",\n",
    "}\n",
    "\n",
    "METRICS_MAPPING = {\n",
    "    \"auroc\": \"AUROC\",\n",
    "    \"auprc\": \"AUPRC\",\n",
    "    \"normalized_auprc\": \"Normalized AUPRC\",\n",
    "    \"minpse\": \"Min(+P, Se)\",\n",
    "    \"recall\": \"Sensitivity (Recall)\",\n",
    "    \"specificity\": \"Specificity\",\n",
    "    \"precision\": \"Precision\",\n",
    "    \"f1_score\": \"F1 Score\",\n",
    "    \"accuracy\": \"Accuracy\",\n",
    "    \"balanced_accuracy\": \"Balanced Accuracy\",\n",
    "    \"mcc\": \"MCC\",\n",
    "    \"kappa\": \"Cohen's Kappa\",\n",
    "}\n",
    "\n",
    "MODEL_CONFIG_PATH_MAPPING = {\n",
    "    \"configs/model_configs/Gemma34BModel.yaml\": \"Gemma3\",\n",
    "    \"configs/model_configs/MedGemma4bModel.yaml\": \"MedGemma\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9427faac",
   "metadata": {},
   "source": [
    "## Input Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4caa7f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility to generate all valid LLM model/prompting_id input paths\n",
    "def get_llm_prompting_id_paths(llms_root_dir, prompting_id_display_mapping, allowed_models=None, allowed_prompting_ids=None):\n",
    "    \"\"\"\n",
    "    Scan the LLMs results directory and return all valid (model, prompting_id) subdirectory paths.\n",
    "    Only includes subdirectories that match a key in PROMPTING_ID_DISPLAY_MAPPING.\n",
    "    Optionally filter by allowed_models and/or allowed_prompting_ids.\n",
    "    Args:\n",
    "        llms_root_dir (str): Path to the root LLMs directory (e.g., .../results_benchmark/llms)\n",
    "        prompting_id_display_mapping (dict): Mapping of valid prompting_id subdirectory names\n",
    "        allowed_models (list, optional): List of allowed model directory names (str)\n",
    "        allowed_prompting_ids (list, optional): List of allowed prompting_id subdirectory names (str)\n",
    "    Returns:\n",
    "        list: List of full paths for all valid (model, prompting_id) combinations\n",
    "    \"\"\"\n",
    "    valid_prompting_ids = set(prompting_id_display_mapping.keys())\n",
    "    if allowed_prompting_ids is not None:\n",
    "        valid_prompting_ids = valid_prompting_ids & set(allowed_prompting_ids)\n",
    "    all_paths = []\n",
    "    if not os.path.isdir(llms_root_dir):\n",
    "        print(f\"LLMs root directory not found: {llms_root_dir}\")\n",
    "        return all_paths\n",
    "    for model_name in os.listdir(llms_root_dir):\n",
    "        if allowed_models is not None and model_name not in allowed_models:\n",
    "            continue\n",
    "        model_dir = os.path.join(llms_root_dir, model_name)\n",
    "        if not os.path.isdir(model_dir):\n",
    "            continue\n",
    "        for subdir in os.listdir(model_dir):\n",
    "            if subdir not in valid_prompting_ids:\n",
    "                continue\n",
    "            subdir_path = os.path.join(model_dir, subdir)\n",
    "            if os.path.isdir(subdir_path):\n",
    "                all_paths.append(subdir_path)\n",
    "    return all_paths\n",
    "\n",
    "\n",
    "def get_model_name_from_config(config_path):\n",
    "    try:\n",
    "        with open(config_path, \"r\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        load_models = config.get(\"load_models\", [])\n",
    "        if isinstance(load_models, list) and load_models:\n",
    "            model_config_path = load_models[0]\n",
    "            return MODEL_CONFIG_PATH_MAPPING.get(model_config_path, None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading config for model name: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38d67b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total output folders (Baselines + LLMs): 68\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/baseline_models/20250603_160304_CNN\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/baseline_models/20250603_192812_GRU\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/baseline_models/20250604_094650_LSTM\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/baseline_models/20250617_131406_InceptionTime\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/baseline_models/20250618_192129_XGBoost\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/baseline_models/20250619_004541_RandomForest\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/gemma34b/clinical_workflow_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/gemma34b/hybrid_reasoning_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/gemma34b/sarvari_2024_aggregation_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/gemma34b/zhu_2024b_one_shot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/gemma34b/zhu_2024a_cot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/gemma34b/zhu_2024c_categorization_summary_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/gemma34b/collaborative_reasoning_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/gemma34b/liu_2023_few_shot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/gemma34b/zhu_2024b_zero_shot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/o3/hybrid_reasoning_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/o3/zhu_2024b_zero_shot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/claudesonnet4/hybrid_reasoning_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/claudesonnet4/zhu_2024b_zero_shot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/llama3p18b/clinical_workflow_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/llama3p18b/hybrid_reasoning_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/llama3p18b/sarvari_2024_aggregation_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/llama3p18b/zhu_2024b_one_shot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/llama3p18b/zhu_2024a_cot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/llama3p18b/zhu_2024c_categorization_summary_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/llama3p18b/collaborative_reasoning_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/llama3p18b/liu_2023_few_shot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/llama3p18b/zhu_2024b_zero_shot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/medgemma4b/clinical_workflow_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/medgemma4b/hybrid_reasoning_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/medgemma4b/sarvari_2024_aggregation_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/medgemma4b/zhu_2024b_one_shot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/medgemma4b/zhu_2024a_cot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/medgemma4b/zhu_2024c_categorization_summary_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/medgemma4b/collaborative_reasoning_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/medgemma4b/liu_2023_few_shot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/medgemma4b/zhu_2024b_zero_shot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/deepseekr1llama8b/clinical_workflow_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/deepseekr1llama8b/hybrid_reasoning_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/deepseekr1llama8b/sarvari_2024_aggregation_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/deepseekr1llama8b/zhu_2024b_one_shot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/deepseekr1llama8b/zhu_2024a_cot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/deepseekr1llama8b/zhu_2024c_categorization_summary_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/deepseekr1llama8b/collaborative_reasoning_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/deepseekr1llama8b/liu_2023_few_shot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/deepseekr1llama8b/zhu_2024b_zero_shot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/gemini2p5pro/hybrid_reasoning_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/gemini2p5pro/zhu_2024b_zero_shot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/grok4/hybrid_reasoning_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/grok4/zhu_2024b_zero_shot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/mistral7b/clinical_workflow_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/mistral7b/hybrid_reasoning_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/mistral7b/sarvari_2024_aggregation_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/mistral7b/zhu_2024b_one_shot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/mistral7b/zhu_2024a_cot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/mistral7b/zhu_2024c_categorization_summary_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/mistral7b/collaborative_reasoning_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/mistral7b/liu_2023_few_shot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/mistral7b/zhu_2024b_zero_shot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/gemini2p5flash/clinical_workflow_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/gemini2p5flash/hybrid_reasoning_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/gemini2p5flash/sarvari_2024_aggregation_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/gemini2p5flash/zhu_2024b_one_shot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/gemini2p5flash/zhu_2024a_cot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/gemini2p5flash/zhu_2024c_categorization_summary_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/gemini2p5flash/collaborative_reasoning_agent_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/gemini2p5flash/liu_2023_few_shot_preprocessor\n",
      "/Users/sophiaehlers/Documents/pulse_agents/output/llms/gemini2p5flash/zhu_2024b_zero_shot_preprocessor\n"
     ]
    }
   ],
   "source": [
    "# Get parent directory of current working directory\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "results_path = os.path.join(parent_dir, \"output\")\n",
    "\n",
    "# Add all baseline model paths\n",
    "outputfolder_path_list = [\n",
    "    os.path.join(results_path, r\"baseline_models/20250603_160304_CNN\"),\n",
    "    os.path.join(results_path, r\"baseline_models/20250603_192812_GRU\"),\n",
    "    os.path.join(results_path, r\"baseline_models/20250604_094650_LSTM\"),\n",
    "    os.path.join(results_path, r\"baseline_models/20250617_131406_InceptionTime\"),\n",
    "    os.path.join(results_path, r\"baseline_models/20250618_192129_XGBoost\"),\n",
    "    os.path.join(results_path, r\"baseline_models/20250619_004541_RandomForest\"),\n",
    "]\n",
    "\n",
    "# Dynamically add all valid LLM model/prompting_id paths\n",
    "llms_root = os.path.join(results_path, \"llms\")\n",
    "llm_prompting_paths = get_llm_prompting_id_paths(\n",
    "    llms_root,\n",
    "    PROMPTING_ID_DISPLAY_MAPPING,\n",
    "    allowed_models=[\n",
    "        \"o3\",\n",
    "        \"claudesonnet4\",\n",
    "        \"grok4\",\n",
    "        \"gemini2p5pro\",\n",
    "        \"gemini2p5flash\",\n",
    "        \"llama3p18b\",\n",
    "        \"deepseekr1llama8b\",\n",
    "        \"mistral7b\",\n",
    "        \"gemma34b\",\n",
    "        \"medgemma4b\",\n",
    "    ],\n",
    "    allowed_prompting_ids=[\n",
    "        \"sarvari_2024_aggregation_preprocessor\",\n",
    "        \"zhu_2024b_zero_shot_preprocessor\",\n",
    "        \"zhu_2024b_one_shot_preprocessor\",\n",
    "        \"liu_2023_few_shot_preprocessor\",\n",
    "        \"zhu_2024a_cot_preprocessor\",\n",
    "        \"zhu_2024c_categorization_summary_agent_preprocessor\",\n",
    "        \"collaborative_reasoning_agent_preprocessor\",\n",
    "        \"clinical_workflow_agent_preprocessor\",\n",
    "        \"hybrid_reasoning_agent_preprocessor\",\n",
    "    ]\n",
    ")\n",
    "outputfolder_path_list += llm_prompting_paths\n",
    "print(f\"Total output folders (Baselines + LLMs): {len(outputfolder_path_list)}\")\n",
    "for path in outputfolder_path_list:\n",
    "    print(path)\n",
    "\n",
    "# Global output directory for all visualizations\n",
    "OUTPUT_BASE_DIR = os.path.join(\"..\", \"visualizations\", \"benchmark_baseline_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a79c15d",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5757ab3b",
   "metadata": {},
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4583d2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_files(outputfolder_path_list):\n",
    "    \"\"\"\n",
    "    Categorize files in the output folders into metrics report files, metadata files, and log files.\n",
    "\n",
    "    Args:\n",
    "        outputfolder_path_list (list): List of output folder paths.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing categorized files.\n",
    "    \"\"\"\n",
    "    file_list = []\n",
    "    file_sources = {}  # Track which folder each file came from\n",
    "    \n",
    "    for outputfolder_path in outputfolder_path_list:\n",
    "        folder_files = glob.glob(os.path.join(outputfolder_path, \"*\"))\n",
    "        for file_path in folder_files:\n",
    "            if file_path in file_sources:\n",
    "                print(f\"⚠️  Duplicate file found:\")\n",
    "                print(f\"     File: {file_path}\")\n",
    "                print(f\"     First found in: {file_sources[file_path]}\")\n",
    "                print(f\"     Also found in: {outputfolder_path}\")\n",
    "            else:\n",
    "                file_sources[file_path] = outputfolder_path\n",
    "        file_list.extend(folder_files)\n",
    "\n",
    "    # Remove duplicates to ensure unique file paths\n",
    "    file_list = list(set(file_list))\n",
    "\n",
    "    categorized_files = {\n",
    "        \"metrics_report_files\": [f for f in file_list if \"metrics_report\" in f],\n",
    "        \"metadata_files\": [f for f in file_list if \"metadata\" in f],\n",
    "        \"log_files\": [f for f in file_list if \"log\" in f],\n",
    "        \"config_files\": [f for f in file_list if \"config\" in f],\n",
    "    }\n",
    "\n",
    "    print(f\"Found {len(categorized_files['metrics_report_files'])} metrics report files, {len(categorized_files['metadata_files'])} metadata files, {len(categorized_files['log_files'])} log files.\")\n",
    "    return categorized_files\n",
    "\n",
    "\n",
    "def load_metadata(metadata_path_list):\n",
    "    \"\"\"\n",
    "    Load metadata from CSV files into a DataFrame.\n",
    "    Enhanced version with better error handling and path parsing.\n",
    "    Adds a 'prompting_id' column for LLM models using the config yaml in the same directory.\n",
    "    Adds an 'is_agent' column: True if unmapped prompting_id contains 'agent', else False.\n",
    "    For LLM metadata files, if the column 'System Message Index' exists, only keep rows where its value is 1.\n",
    "    For ambiguous Gemma3/MedGemma cases, uses config_copy.yaml to assign correct model name.\n",
    "    \"\"\"\n",
    "    df_mdata = pd.DataFrame()\n",
    "\n",
    "    for m_path in metadata_path_list:\n",
    "        try:\n",
    "            df = pd.read_csv(m_path)\n",
    "            df[\"source_file\"] = m_path\n",
    "\n",
    "            filename = os.path.basename(m_path)\n",
    "            folder_path = os.path.dirname(m_path)\n",
    "            folder_name = os.path.basename(folder_path)\n",
    "            patterns = [\n",
    "                r\"([^_]+)_([^_]+)_([^_]+)_(\\d{8}_\\d{6})_metadata\\.csv$\",\n",
    "                r\"([^_]+)_([^_]+)_([^_]+)_output_metadata\\.csv$\",\n",
    "                r\"([^_]+)_([^_]+)_([^_]+)_metadata\\.csv$\",\n",
    "                r\"([^_]+)_([^_]+)_([^_]+).*metadata\\.csv$\",\n",
    "            ]\n",
    "            extracted = False\n",
    "            timestamp = \"Unknown\"\n",
    "            folder_match = re.search(r\"(\\d{8}_\\d{6})\", folder_name)\n",
    "            if folder_match:\n",
    "                timestamp = folder_match.group(1)\n",
    "            model_name, task, dataset = None, None, None\n",
    "\n",
    "            # Try to extract model_name, task, dataset from filename\n",
    "            for i, pattern in enumerate(patterns):\n",
    "                match = re.search(pattern, filename)\n",
    "                if match:\n",
    "                    if i == 0:\n",
    "                        model_name, task, dataset, timestamp = match.groups()\n",
    "                    else:\n",
    "                        model_name, task, dataset = match.groups()\n",
    "                    if model_name.endswith(\"Model\"):\n",
    "                        model_name = model_name[:-5]\n",
    "                    extracted = True\n",
    "                    break\n",
    "\n",
    "            # --- Robust model name assignment for Gemma3/MedGemma ---\n",
    "            # If ambiguous (model_name == \"Gemma3\"), use config_copy.yaml to resolve\n",
    "            if model_name == \"Gemma3\":\n",
    "                config_files = [f for f in os.listdir(folder_path) if f.startswith(\"config_copy\") and f.endswith(\".yaml\")]\n",
    "                if config_files:\n",
    "                    config_path = os.path.join(folder_path, config_files[0])\n",
    "                    resolved_name = get_model_name_from_config(config_path)\n",
    "                    if resolved_name:\n",
    "                        model_name = resolved_name\n",
    "\n",
    "            df[\"model_name\"] = model_name\n",
    "            df[\"task\"] = task\n",
    "            df[\"dataset\"] = dataset\n",
    "            df[\"timestamp\"] = timestamp\n",
    "\n",
    "            # Add prompting_id column (empty by default)\n",
    "            df[\"prompting_id\"] = \"\" # Use empty string for missing prompting_id\n",
    "\n",
    "            # If LLM, try to get prompting_id from config yaml in the same folder\n",
    "            llm_names = list(LLM_MODELS.keys()) + list(LLM_MODELS.values())\n",
    "            if model_name in llm_names:\n",
    "                config_files = [f for f in os.listdir(folder_path) if f.endswith(\".yaml\") or f.endswith(\".yml\")]\n",
    "                for config_file in config_files:\n",
    "                    config_path = os.path.join(folder_path, config_file)\n",
    "                    try:\n",
    "                        with open(config_path, \"r\") as f:\n",
    "                            config = yaml.safe_load(f)\n",
    "                        # Try to get prompting_ids from config\n",
    "                        prompting_ids = None\n",
    "                        if \"prompting\" in config and \"prompting_ids\" in config[\"prompting\"]:\n",
    "                            prompting_ids = config[\"prompting\"][\"prompting_ids\"]\n",
    "                        if prompting_ids:\n",
    "                            # Use first prompting_id if list, else as string\n",
    "                            if isinstance(prompting_ids, list):\n",
    "                                df[\"prompting_id\"] = prompting_ids[0]\n",
    "                            else:\n",
    "                                df[\"prompting_id\"] = prompting_ids\n",
    "                            break\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "            # Add is_agent column: True if unmapped prompting_id contains 'agent', else False\n",
    "            def compute_is_agent_row(row):\n",
    "                if model_name in llm_names and isinstance(row['prompting_id'], str):\n",
    "                    return 'agent' in row['prompting_id'].lower()\n",
    "                return False\n",
    "            df['is_agent'] = df.apply(compute_is_agent_row, axis=1)\n",
    "\n",
    "            df_mdata = pd.concat([df_mdata, df], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading metadata from {m_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not df_mdata.empty:\n",
    "        df_mdata[\"model_type\"] = df_mdata.apply(\n",
    "            lambda row: categorize_model_type(\n",
    "                row[\"model_name\"],\n",
    "                context=f\"file={row.get('source_file', 'unknown')}, row_index={row.name}, model_name={row.get('model_name')}\",\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        df_mdata = unpack_metadata_by_model_type(df_mdata)\n",
    "        df_mdata = apply_task_dataset_mappings(df_mdata)\n",
    "        # Apply all name mappings before ordering\n",
    "        df_mdata = apply_prompting_id_mapping(df_mdata)\n",
    "        df_mdata = map_llm_model_names(df_mdata)\n",
    "        # Now order using display names only\n",
    "        df_mdata = order_dataframe_by_mappings(df_mdata)\n",
    "        # Add model_prompting_id column: LLMs get model_name+prompting_id, others just model_name\n",
    "        def get_model_prompting_id(row):\n",
    "            if row['model_type'] == 'LLM':\n",
    "                return f\"{row['model_name']}, {row['prompting_id']}\" if row['prompting_id'] else row['model_name']\n",
    "            else:\n",
    "                return row[\"model_name\"]\n",
    "        df_mdata[\"model_prompting_id\"] = df_mdata.apply(get_model_prompting_id, axis=1)\n",
    "        print(\n",
    "            f\"Loaded metadata: {df_mdata.shape[0]} rows, {df_mdata.shape[1]} columns.\"\n",
    "        )\n",
    "        return df_mdata\n",
    "\n",
    "def apply_prompting_id_mapping(df_metadata):\n",
    "    \"\"\"\n",
    "    Map the prompting_id column to display names using PROMPTING_ID_DISPLAY_MAPPING.\n",
    "    \"\"\"\n",
    "    if 'prompting_id' in df_metadata.columns:\n",
    "        df_metadata['prompting_id'] = df_metadata['prompting_id'].map(PROMPTING_ID_DISPLAY_MAPPING).fillna(df_metadata['prompting_id'])\n",
    "    return df_metadata\n",
    "\n",
    "\n",
    "# Map LLM model names to display names using the LLM_MODELS dictionary\n",
    "def map_llm_model_names(df_metadata):\n",
    "    \"\"\"\n",
    "    Map LLM model names to their display names using the LLM_MODELS dictionary.\n",
    "    \"\"\"\n",
    "    if 'model_name' in df_metadata.columns:\n",
    "        df_metadata['model_name'] = df_metadata['model_name'].replace(LLM_MODELS)\n",
    "    return df_metadata\n",
    "\n",
    "\n",
    "def categorize_model_type(model_name, context=None):\n",
    "    \"\"\"\n",
    "    Categorize model into convML, convDL, or LLM based on model name.\n",
    "    \"\"\"\n",
    "    if model_name in CONVML_MODELS:\n",
    "        return \"convML\"\n",
    "    elif model_name in CONVDL_MODELS:\n",
    "        return \"convDL\"\n",
    "    elif model_name in LLM_MODELS.keys() or model_name in LLM_MODELS.values():\n",
    "        return \"LLM\"\n",
    "    else:\n",
    "        msg = f\"Warning: Unknown model type for {model_name}, defaulting to 'Unknown'\"\n",
    "        if context:\n",
    "            msg += f\" | Context: {context}\"\n",
    "        print(msg)\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "def unpack_metadata_by_model_type(df_metadata):\n",
    "    \"\"\"\n",
    "    Unpack metadata based on model type (convDL vs convML vs LLM).\n",
    "    \n",
    "    Args:\n",
    "        df_metadata (DataFrame): Metadata DataFrame with model_type column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Unpacked DataFrame where each row is one sample\n",
    "    \"\"\"\n",
    "    def parse_array_string(array_str):\n",
    "        \"\"\"Parse string representation of numpy array to actual array.\"\"\"\n",
    "        import ast\n",
    "        import numpy as np\n",
    "        try:\n",
    "            # Remove newlines and extra spaces, then evaluate\n",
    "            clean_str = ' '.join(array_str.split())\n",
    "            # Handle numpy array string format\n",
    "            if array_str.startswith('[[') or array_str.startswith('['):\n",
    "                # Convert string to actual array\n",
    "                array_str = array_str.replace('[', '').replace(']', '').replace('\\n', ' ')\n",
    "                values = [float(x) for x in array_str.split() if x.strip()]\n",
    "                return np.array(values)\n",
    "            else:\n",
    "                return ast.literal_eval(array_str)\n",
    "        except:\n",
    "            print(f\"Warning: Could not parse array string: {array_str[:100]}...\")\n",
    "            return None\n",
    "    \n",
    "    unpacked_rows = []\n",
    "    \n",
    "    for idx, row in df_metadata.iterrows():\n",
    "        model_type = row['model_type']\n",
    "        \n",
    "        if model_type == 'convDL':\n",
    "            # convDL models have batch structure - need to unpack predictions and labels\n",
    "            try:\n",
    "                # Parse predictions and labels\n",
    "                predictions = parse_array_string(row['prediction'])\n",
    "                labels = parse_array_string(row['label'])\n",
    "                \n",
    "                # Parse demographic arrays (these are repeated for the batch)\n",
    "                age_array = parse_array_string(row['age'])\n",
    "                sex_array = parse_array_string(row['sex'])\n",
    "                height_array = parse_array_string(row['height'])\n",
    "                weight_array = parse_array_string(row['weight'])\n",
    "                \n",
    "                if predictions is not None and labels is not None:\n",
    "                    # Create individual rows for each prediction/label pair\n",
    "                    for i in range(len(predictions)):\n",
    "                        new_row = row.copy()\n",
    "                        new_row['prediction'] = predictions[i]\n",
    "                        new_row['label'] = labels[i]\n",
    "                        new_row['age'] = age_array[i] if age_array is not None and i < len(age_array) else None\n",
    "                        new_row['sex'] = sex_array[i] if sex_array is not None and i < len(sex_array) else None\n",
    "                        new_row['height'] = height_array[i] if height_array is not None and i < len(height_array) else None\n",
    "                        new_row['weight'] = weight_array[i] if weight_array is not None and i < len(weight_array) else None\n",
    "                        unpacked_rows.append(new_row)\n",
    "                else:\n",
    "                    print(f\"Warning: Could not parse predictions/labels for row {idx}\")\n",
    "                    unpacked_rows.append(row)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error unpacking convDL row {idx}: {e}\")\n",
    "                unpacked_rows.append(row)\n",
    "                \n",
    "        elif model_type == 'convML':\n",
    "            # convML models already have individual sample structure - parse arrays\n",
    "            try:\n",
    "                # Parse the arrays but keep individual values\n",
    "                predictions = parse_array_string(row['prediction'])\n",
    "                labels = parse_array_string(row['label'])\n",
    "                age_array = parse_array_string(row['age'])\n",
    "                sex_array = parse_array_string(row['sex'])\n",
    "                height_array = parse_array_string(row['height'])\n",
    "                weight_array = parse_array_string(row['weight'])\n",
    "                \n",
    "                if predictions is not None and labels is not None:\n",
    "                    # Create individual rows for each sample\n",
    "                    for i in range(len(predictions)):\n",
    "                        new_row = row.copy()\n",
    "                        new_row['prediction'] = predictions[i]\n",
    "                        new_row['label'] = labels[i]\n",
    "                        new_row['age'] = age_array[i] if age_array is not None and i < len(age_array) else None\n",
    "                        new_row['sex'] = sex_array[i] if sex_array is not None and i < len(sex_array) else None\n",
    "                        new_row['height'] = height_array[i] if height_array is not None and i < len(height_array) else None\n",
    "                        new_row['weight'] = weight_array[i] if weight_array is not None and i < len(weight_array) else None\n",
    "                        unpacked_rows.append(new_row)\n",
    "                else:\n",
    "                    print(f\"Warning: Could not parse arrays for row {idx}\")\n",
    "                    unpacked_rows.append(row)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error unpacking convML row {idx}: {e}\")\n",
    "                unpacked_rows.append(row)\n",
    "                \n",
    "        elif model_type == 'LLM':\n",
    "            # LLM models are already unpacked - extract target label and predicted probability\n",
    "            # LLM metadata structure: Input Prompt, Target Label, Predicted Probability, Predicted Diagnosis, \n",
    "            # Predicted Explanation, Tokenization Time, Inference Time, Input Tokens, Output Tokens\n",
    "            new_row = row.copy()\n",
    "            \n",
    "            # Map LLM columns to standard format\n",
    "            if 'Target Label' in row:\n",
    "                new_row['label'] = row['Target Label']\n",
    "            elif 'target_label' in row:\n",
    "                new_row['label'] = row['target_label']\n",
    "            \n",
    "            if 'Predicted Probability' in row:\n",
    "                new_row['prediction'] = row['Predicted Probability']\n",
    "            elif 'predicted_probability' in row:\n",
    "                new_row['prediction'] = row['predicted_probability']\n",
    "            \n",
    "            # Set placeholder demographics - these will be mapped from convML models later\n",
    "            new_row['age'] = None\n",
    "            new_row['sex'] = None\n",
    "            new_row['height'] = None\n",
    "            new_row['weight'] = None\n",
    "            \n",
    "            unpacked_rows.append(new_row)\n",
    "            \n",
    "        else:\n",
    "            # Unknown model type - keep as is\n",
    "            context = f\"file={row.get('source_file', 'unknown')}, row_index={idx}, model_name={row.get('model_name')}\"\n",
    "            print(f\"Warning: Unknown model type '{model_type}' for row {idx} | Context: {context}\")\n",
    "            unpacked_rows.append(row)\n",
    "    \n",
    "    # Create new DataFrame from unpacked rows\n",
    "    if unpacked_rows:\n",
    "        df_unpacked = pd.DataFrame(unpacked_rows)\n",
    "        print(f\"Unpacked {len(df_metadata)} batch rows into {len(df_unpacked)} individual sample rows\")\n",
    "        \n",
    "        # Show breakdown by model type\n",
    "        model_type_counts = df_unpacked['model_type'].value_counts()\n",
    "        print(f\"Breakdown by model type:\")\n",
    "        for model_type, count in model_type_counts.items():\n",
    "            print(f\"  {model_type}: {count:,} samples\")\n",
    "        \n",
    "        return df_unpacked\n",
    "    else:\n",
    "        print(\"Warning: No rows were successfully unpacked\")\n",
    "        return df_metadata\n",
    "\n",
    "\n",
    "def apply_task_dataset_mappings(df_metadata):\n",
    "    \"\"\"\n",
    "    Apply task and dataset name mappings according to global TASK_MAPPING and DATASET_MAPPING.\n",
    "    Args:\n",
    "        df_metadata (DataFrame): Metadata DataFrame\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with mapped task and dataset names\n",
    "    \"\"\"\n",
    "    # Create reverse mappings to find keys from current values\n",
    "    task_reverse_mapping = {}\n",
    "    for key, display_name in TASK_MAPPING.items():\n",
    "        task_reverse_mapping[key] = display_name\n",
    "        task_reverse_mapping[key.lower()] = display_name\n",
    "        task_reverse_mapping[key.capitalize()] = display_name\n",
    "        task_reverse_mapping[key.upper()] = display_name\n",
    "    dataset_reverse_mapping = {}\n",
    "    for key, display_name in DATASET_MAPPING.items():\n",
    "        dataset_reverse_mapping[key] = display_name\n",
    "        dataset_reverse_mapping[key.lower()] = display_name\n",
    "        dataset_reverse_mapping[key.capitalize()] = display_name\n",
    "        dataset_reverse_mapping[key.upper()] = display_name\n",
    "    # Apply task mappings\n",
    "    original_tasks = df_metadata['task'].unique()\n",
    "    df_metadata['task'] = df_metadata['task'].map(task_reverse_mapping).fillna(df_metadata['task'])\n",
    "    mapped_tasks = df_metadata['task'].unique()\n",
    "    # Apply dataset mappings\n",
    "    original_datasets = df_metadata['dataset'].unique()\n",
    "    df_metadata['dataset'] = df_metadata['dataset'].map(dataset_reverse_mapping).fillna(df_metadata['dataset'])\n",
    "    mapped_datasets = df_metadata['dataset'].unique()\n",
    "    return df_metadata\n",
    "\n",
    "\n",
    "def order_dataframe_by_mappings(df_metadata):\n",
    "    \"\"\"\n",
    "    Order the dataframe according to the order defined in global mappings.\n",
    "    Hierarchical ordering: model_type (convML → convDL → LLM) → model_name → prompting_id → dataset → task\n",
    "    NOTE: This function expects display names to already be mapped before calling.\n",
    "    Args:\n",
    "        df_metadata (DataFrame): Metadata DataFrame with mapped display names\n",
    "    Returns:\n",
    "        DataFrame: Ordered DataFrame\n",
    "    \"\"\"\n",
    "    # Define order based on mappings\n",
    "    task_order = list(TASK_MAPPING.values())\n",
    "    dataset_order = list(DATASET_MAPPING.values())\n",
    "    \n",
    "    # Create model type ordering\n",
    "    model_type_order = ['convML', 'convDL', 'LLM']\n",
    "    \n",
    "    # Create model name ordering within each model type - use display names only\n",
    "    convml_model_order = CONVML_MODELS\n",
    "    convdl_model_order = CONVDL_MODELS\n",
    "    # For LLM models, use only the display names\n",
    "    llm_model_order = list(LLM_MODELS.values())\n",
    "    \n",
    "    # Create prompting_id ordering (for LLMs) - use only display names\n",
    "    prompting_id_order = list(PROMPTING_ID_DISPLAY_MAPPING.values())\n",
    "    \n",
    "    # Create categorical columns for proper sorting\n",
    "    df_metadata['model_type'] = pd.Categorical(\n",
    "        df_metadata['model_type'],\n",
    "        categories=model_type_order,\n",
    "        ordered=True\n",
    "    )\n",
    "    \n",
    "    # Create a combined model ordering with display names only\n",
    "    all_model_order = convml_model_order + convdl_model_order + llm_model_order\n",
    "    df_metadata['model_name'] = pd.Categorical(\n",
    "        df_metadata['model_name'], \n",
    "        categories=all_model_order,\n",
    "        ordered=True\n",
    "    )\n",
    "    \n",
    "    # Create prompting_id categorical (empty string for non-LLM models)\n",
    "    if 'prompting_id' in df_metadata.columns:\n",
    "        # Add empty string to the beginning and include only display names\n",
    "        prompting_id_categories = [''] + prompting_id_order\n",
    "        df_metadata['prompting_id'] = pd.Categorical(\n",
    "            df_metadata['prompting_id'],\n",
    "            categories=prompting_id_categories,\n",
    "            ordered=True\n",
    "        )\n",
    "    \n",
    "    df_metadata['task'] = pd.Categorical(\n",
    "        df_metadata['task'],\n",
    "        categories=task_order,\n",
    "        ordered=True\n",
    "    )\n",
    "    df_metadata['dataset'] = pd.Categorical(\n",
    "        df_metadata['dataset'],\n",
    "        categories=dataset_order,\n",
    "        ordered=True\n",
    "    )\n",
    "    \n",
    "    # Sort hierarchically: model_type → model_name → prompting_id → dataset → task\n",
    "    sort_columns = ['model_type', 'model_name']\n",
    "    if 'prompting_id' in df_metadata.columns:\n",
    "        sort_columns.append('prompting_id')\n",
    "    sort_columns.extend(['dataset', 'task'])\n",
    "    \n",
    "    df_metadata = df_metadata.sort_values(sort_columns, ignore_index=True)\n",
    "    return df_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77790317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sample_index_to_metadata(df_metadata):\n",
    "    \"\"\"\n",
    "    Add sample_index to metadata to ensure consistent sample alignment across models.\n",
    "    For convML/convDL: index by (model_name, task, dataset).\n",
    "    For LLM: index by (model_name, prompting_id, task, dataset).\n",
    "    For agent LLMs (is_agent=True): sample_index increments at Step Name == 1 or Step Name == 'SAMPLE_METADATA'.\n",
    "    Uses original_row_index to guarantee sample_index follows original row order.\n",
    "    Args:\n",
    "        df_metadata (DataFrame): Metadata DataFrame after unpacking\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with sample_index column added\n",
    "    \"\"\"\n",
    "    df_result = df_metadata.copy()\n",
    "    # convML/convDL\n",
    "    mask_ml = df_result['model_type'].isin(['convML', 'convDL'])\n",
    "    group_cols_ml = ['model_name', 'task', 'dataset']\n",
    "    for keys, group in df_result[mask_ml].groupby(group_cols_ml, observed=True):\n",
    "        idx = group.index\n",
    "        # Sort by original_row_index to preserve original order\n",
    "        group_sorted = group.sort_values('original_row_index')\n",
    "        sample_indices = np.arange(len(group_sorted)).astype(int)\n",
    "        df_result.loc[group_sorted.index, 'sample_index'] = sample_indices\n",
    "    # LLM\n",
    "    mask_llm = df_result['model_type'] == 'LLM'\n",
    "    group_cols_llm = ['model_name', 'prompting_id', 'task', 'dataset']\n",
    "    for keys, group in df_result[mask_llm].groupby(group_cols_llm, observed=True):\n",
    "        idx = group.index\n",
    "        is_agent = group['is_agent'].iloc[0] if 'is_agent' in group else False\n",
    "        # Sort by original_row_index to preserve original order\n",
    "        group_sorted = group.sort_values('original_row_index')\n",
    "\n",
    "        if is_agent:\n",
    "            sample_indices = np.full(len(group_sorted), -1, dtype=int)\n",
    "            current_index = -1\n",
    "            prompting_id = group['prompting_id'].iloc[0] if 'prompting_id' in group else ''\n",
    "\n",
    "            for i, (_, row) in enumerate(group_sorted.iterrows()):\n",
    "                step_name = row.get('Step Name', None)\n",
    "                step_number = row.get('Step Number', None)\n",
    "\n",
    "                # Different increment logic based on prompting_id\n",
    "                if prompting_id == 'SumAgent':\n",
    "                    # For SumAgent: increment at Step Number == 1\n",
    "                    if step_number == 1:\n",
    "                        current_index += 1\n",
    "                else:\n",
    "                    # For other agents: increment at Step Name == 'SAMPLE_METADATA'\n",
    "                    if isinstance(step_name, str) and step_name.strip().upper() == 'SAMPLE_METADATA':\n",
    "                        current_index += 1\n",
    "\n",
    "                sample_indices[i] = current_index\n",
    "            df_result.loc[group_sorted.index, 'sample_index'] = sample_indices.astype(int)\n",
    "        else:\n",
    "            sample_indices = np.arange(len(group_sorted)).astype(int)\n",
    "            df_result.loc[group_sorted.index, 'sample_index'] = sample_indices\n",
    "    # Ensure column is int type (if possible)\n",
    "    if 'sample_index' in df_result.columns:\n",
    "        try:\n",
    "            df_result['sample_index'] = df_result['sample_index'].astype(int)\n",
    "        except Exception:\n",
    "            pass\n",
    "    print(f\"Sample indices added for all model-type groups (using original_row_index order).\")\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "deffa8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_control_check_sample_counts(df_metadata):\n",
    "    \"\"\"\n",
    "    Quality control function to check if task-dataset combinations have \n",
    "    the same number of rows for each model_name.\n",
    "    \n",
    "    Args:\n",
    "        df_metadata (DataFrame): Metadata DataFrame with unpacked samples\n",
    "        \n",
    "    Returns:\n",
    "        dict: Quality control results with warnings and statistics\n",
    "    \"\"\"\n",
    "    qc_results = {\n",
    "        'passed': True,\n",
    "        'warnings': [],\n",
    "        'statistics': {},\n",
    "        'inconsistencies': []\n",
    "    }\n",
    "    task_dataset_combinations = df_metadata.groupby(['task', 'dataset'])\n",
    "    for (task, dataset), group in task_dataset_combinations:\n",
    "        model_counts = group['model_name'].value_counts().sort_index()\n",
    "        combination_key = f\"{task}_{dataset}\"\n",
    "        qc_results['statistics'][combination_key] = model_counts.to_dict()\n",
    "        unique_counts = model_counts.unique()\n",
    "        if len(unique_counts) != 1:\n",
    "            qc_results['passed'] = False\n",
    "            expected_count = model_counts.mode().iloc[0]\n",
    "            inconsistent_models = model_counts[model_counts != expected_count]\n",
    "            inconsistency = {\n",
    "                'task': task,\n",
    "                'dataset': dataset,\n",
    "                'expected_count': expected_count,\n",
    "                'inconsistent_models': inconsistent_models.to_dict()\n",
    "            }\n",
    "            qc_results['inconsistencies'].append(inconsistency)\n",
    "            warning_msg = f\"Inconsistent sample counts in {task}-{dataset}: {inconsistent_models.to_dict()}\"\n",
    "            qc_results['warnings'].append(warning_msg)\n",
    "    total_combinations = len(task_dataset_combinations)\n",
    "    inconsistent_combinations = len(qc_results['inconsistencies'])\n",
    "    consistent_combinations = total_combinations - inconsistent_combinations\n",
    "    print(f\"QC: {consistent_combinations} consistent, {inconsistent_combinations} inconsistent task-dataset combinations.\")\n",
    "    demographics_qc = check_demographics_overlap_convml(df_metadata)\n",
    "    qc_results['demographics_check'] = demographics_qc\n",
    "    if not demographics_qc['passed']:\n",
    "        qc_results['passed'] = False\n",
    "        qc_results['warnings'].extend(demographics_qc['warnings'])\n",
    "    return qc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "deffa8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_convml_demographics_to_all_models(df_metadata):\n",
    "    \"\"\"\n",
    "    Map demographics data from convML models to all convDL and LLM models using sample_index for alignment.\n",
    "    For LLMs, mapping is performed for each unique (model_name, prompting_id, task, dataset) group.\n",
    "    Args:\n",
    "        df_metadata (DataFrame): Metadata DataFrame with mixed demographics and sample_index column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Updated DataFrame with consistent demographics across all models\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Demographics Mapping Step ---\")\n",
    "    df_result = df_metadata.copy()\n",
    "    demographic_cols = ['age', 'sex', 'height', 'weight']\n",
    "    required_cols = demographic_cols + ['sample_index']\n",
    "    missing_cols = [col for col in required_cols if col not in df_result.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"   ❌ Missing required columns: {missing_cols}\")\n",
    "        return df_result\n",
    "    mapping_stats = {\n",
    "        'combinations_processed': 0,\n",
    "        'convdl_models_updated': set(),\n",
    "        'llm_models_updated': set(),\n",
    "        'samples_updated': 0,\n",
    "        'successful_combinations': 0,\n",
    "        'failed_combinations': []\n",
    "    }\n",
    "    combinations = df_result.groupby(['task', 'dataset'], observed=True).size().index.tolist()\n",
    "    for task, dataset in combinations:\n",
    "        combination_mask = (df_result['task'] == task) & (df_result['dataset'] == dataset)\n",
    "        combination_data = df_result[combination_mask].copy()\n",
    "        convml_data = combination_data[combination_data['model_type'] == 'convML']\n",
    "        convdl_data = combination_data[combination_data['model_type'] == 'convDL']\n",
    "        llm_data = combination_data[combination_data['model_type'] == 'LLM']\n",
    "        if convml_data.empty:\n",
    "            continue\n",
    "        if convdl_data.empty and llm_data.empty:\n",
    "            continue\n",
    "        mapping_stats['combinations_processed'] += 1\n",
    "        convml_models = convml_data['model_name'].unique()\n",
    "        reference_model = convml_models[0]\n",
    "        reference_data = convml_data[convml_data['model_name'] == reference_model].set_index('sample_index')\n",
    "        combination_success = True\n",
    "        # Update convDL models as before\n",
    "        if not convdl_data.empty:\n",
    "            convdl_models = convdl_data['model_name'].unique()\n",
    "            for convdl_model in convdl_models:\n",
    "                convdl_model_data = convdl_data[convdl_data['model_name'] == convdl_model].set_index('sample_index')\n",
    "                if not reference_data.index.equals(convdl_model_data.index):\n",
    "                    mapping_stats['failed_combinations'].append(f\"{task}-{dataset} convDL {convdl_model}\")\n",
    "                    combination_success = False\n",
    "                    continue\n",
    "                for sample_idx in reference_data.index:\n",
    "                    row_mask = (df_result['model_name'] == convdl_model) & \\\n",
    "                              (df_result['task'] == task) & \\\n",
    "                              (df_result['dataset'] == dataset) & \\\n",
    "                              (df_result['sample_index'] == sample_idx)\n",
    "                    for col in demographic_cols:\n",
    "                        df_result.loc[row_mask, col] = reference_data.loc[sample_idx, col]\n",
    "                mapping_stats['convdl_models_updated'].add(convdl_model)\n",
    "                mapping_stats['samples_updated'] += len(convdl_model_data)\n",
    "        # Update LLM models for each (model_name, prompting_id, task, dataset)\n",
    "        if not llm_data.empty:\n",
    "            llm_group_cols = ['model_name', 'prompting_id', 'task', 'dataset']\n",
    "            llm_groups = llm_data.groupby(llm_group_cols, observed=True)\n",
    "            for llm_keys, llm_group in llm_groups:\n",
    "                llm_model, prompting_id, llm_task, llm_dataset = llm_keys\n",
    "                if llm_task != task or llm_dataset != dataset:\n",
    "                    continue\n",
    "                llm_model_data = llm_group.set_index('sample_index')\n",
    "                is_agent = llm_group['is_agent'].iloc[0] if 'is_agent' in llm_group else False\n",
    "\n",
    "                if is_agent:\n",
    "                    # For agent, map demographics to all rows with sample_index in reference_data\n",
    "                    valid_indices = reference_data.index.intersection(llm_model_data.index)\n",
    "                    for sample_idx in valid_indices:\n",
    "                        row_mask = (df_result['model_name'] == llm_model) & \\\n",
    "                                  (df_result['task'] == task) & \\\n",
    "                                  (df_result['dataset'] == dataset) & \\\n",
    "                                  (df_result['sample_index'] == sample_idx) & \\\n",
    "                                  (df_result['prompting_id'] == prompting_id)\n",
    "                        for col in demographic_cols:\n",
    "                            df_result.loc[row_mask, col] = reference_data.loc[sample_idx, col]\n",
    "                    mapping_stats['llm_models_updated'].add((llm_model, prompting_id, task, dataset))\n",
    "                    mapping_stats['samples_updated'] += len(valid_indices)\n",
    "                else:\n",
    "                    if not reference_data.index.equals(llm_model_data.index):\n",
    "                        fail_str = f\"{task}-{dataset} LLM {llm_model} (prompting_id={prompting_id})\"\n",
    "                        mapping_stats['failed_combinations'].append(fail_str)\n",
    "                        combination_success = False\n",
    "                        continue\n",
    "                    for sample_idx in reference_data.index:\n",
    "                        row_mask = (df_result['model_name'] == llm_model) & \\\n",
    "                                  (df_result['task'] == task) & \\\n",
    "                                  (df_result['dataset'] == dataset) & \\\n",
    "                                  (df_result['sample_index'] == sample_idx) & \\\n",
    "                                  (df_result['prompting_id'] == prompting_id)\n",
    "                        for col in demographic_cols:\n",
    "                            df_result.loc[row_mask, col] = reference_data.loc[sample_idx, col]\n",
    "                    mapping_stats['llm_models_updated'].add((llm_model, prompting_id, task, dataset))\n",
    "                    mapping_stats['samples_updated'] += len(llm_model_data)\n",
    "        if combination_success:\n",
    "            mapping_stats['successful_combinations'] += 1\n",
    "    total_convdl_updated = len(mapping_stats['convdl_models_updated'])\n",
    "    total_llm_updated = len(mapping_stats['llm_models_updated'])\n",
    "    print(f\"   Processed {mapping_stats['combinations_processed']} task-dataset combinations.\")\n",
    "    print(f\"   Updated {total_convdl_updated} convDL models and {total_llm_updated} LLM model/prompting_id/task/dataset combinations.\")\n",
    "    print(f\"   Total samples updated: {mapping_stats['samples_updated']:,}\")\n",
    "    if mapping_stats['failed_combinations']:\n",
    "        print(f\"   Failed to map {len(mapping_stats['failed_combinations'])} model combinations.\")\n",
    "    verification_result = verify_demographics_mapping(df_result)\n",
    "    if verification_result['perfect_consistency']:\n",
    "        print(f\"   Demographics mapping verification PASSED: all combinations consistent.\")\n",
    "    else:\n",
    "        print(f\"   Demographics mapping verification FAILED: {len(verification_result['inconsistent_combinations'])} inconsistent combinations.\")\n",
    "    print(\"--- End Demographics Mapping ---\\n\")\n",
    "    return df_result\n",
    "\n",
    "\n",
    "def verify_demographics_mapping(df_metadata):\n",
    "    \"\"\"\n",
    "    Verify that all models now have identical demographics for each group used in mapping.\n",
    "    For convML/convDL: check (model_name, task, dataset).\n",
    "    For LLM: check (model_name, prompting_id, task, dataset).\n",
    "    Returns:\n",
    "        dict: Verification results\n",
    "    \"\"\"\n",
    "    verification_results = {\n",
    "        'perfect_consistency': True,\n",
    "        'inconsistent_combinations': [],\n",
    "        'consistent_combinations': 0\n",
    "    }\n",
    "    demographic_cols = ['age', 'sex', 'height', 'weight']\n",
    "    # convML/convDL\n",
    "    mask_ml = df_metadata['model_type'].isin(['convML', 'convDL'])\n",
    "    group_cols_ml = ['task', 'dataset']\n",
    "    for (task, dataset), group in df_metadata[mask_ml].groupby(group_cols_ml, observed=True):\n",
    "        models = group['model_name'].unique()\n",
    "        if len(models) < 2:\n",
    "            continue\n",
    "        reference_model = models[0]\n",
    "        reference_data = group[group['model_name'] == reference_model].set_index('sample_index')[demographic_cols]\n",
    "        all_models_consistent = True\n",
    "        inconsistent_models = []\n",
    "        for model in models[1:]:\n",
    "            model_data = group[group['model_name'] == model].set_index('sample_index')[demographic_cols]\n",
    "            if not reference_data.index.equals(model_data.index):\n",
    "                all_models_consistent = False\n",
    "                inconsistent_models.append(model)\n",
    "                continue\n",
    "            demographics_match = True\n",
    "            for col in demographic_cols:\n",
    "                if col == 'sex':\n",
    "                    if not (reference_data[col] == model_data[col]).all():\n",
    "                        demographics_match = False\n",
    "                        break\n",
    "                else:\n",
    "                    if not np.allclose(reference_data[col], model_data[col], rtol=1e-6, equal_nan=True):\n",
    "                        demographics_match = False\n",
    "                        break\n",
    "            if not demographics_match:\n",
    "                all_models_consistent = False\n",
    "                inconsistent_models.append(model)\n",
    "        if all_models_consistent:\n",
    "            verification_results['consistent_combinations'] += 1\n",
    "        else:\n",
    "            verification_results['perfect_consistency'] = False\n",
    "            verification_results['inconsistent_combinations'].append({\n",
    "                'task': task,\n",
    "                'dataset': dataset,\n",
    "                'reference_model': reference_model,\n",
    "                'inconsistent_models': inconsistent_models\n",
    "            })\n",
    "    # LLM\n",
    "    mask_llm = df_metadata['model_type'] == 'LLM'\n",
    "    group_cols_llm = ['task', 'dataset', 'model_name']\n",
    "    for (task, dataset, model_name), group in df_metadata[mask_llm].groupby(group_cols_llm, observed=True):\n",
    "        prompting_ids = group['prompting_id'].unique()\n",
    "        # Find convML reference row count for this task/dataset\n",
    "        convml_mask = (\n",
    "            (df_metadata['model_type'] == 'convML') &\n",
    "            (df_metadata['task'] == task) &\n",
    "            (df_metadata['dataset'] == dataset)\n",
    "        )\n",
    "        convml_ref_count = None\n",
    "        if convml_mask.any():\n",
    "            convml_ref_count = len(df_metadata[convml_mask]['sample_index'].unique())\n",
    "        # Find all prompting_id groups matching convML reference row count\n",
    "        matching_promptings = []\n",
    "        for pid in prompting_ids:\n",
    "            pid_group = group[group['prompting_id'] == pid]\n",
    "            if convml_ref_count is not None and len(pid_group['sample_index'].unique()) == convml_ref_count:\n",
    "                matching_promptings.append(pid)\n",
    "        # Report only those that do not match convML reference\n",
    "        inconsistent_promptings = []\n",
    "        for pid in prompting_ids:\n",
    "            pid_group = group[group['prompting_id'] == pid]\n",
    "            if convml_ref_count is None or len(pid_group['sample_index'].unique()) != convml_ref_count:\n",
    "                inconsistent_promptings.append(pid)\n",
    "        if len(inconsistent_promptings) == 0:\n",
    "            verification_results['consistent_combinations'] += 1\n",
    "        else:\n",
    "            verification_results['perfect_consistency'] = False\n",
    "            verification_results['inconsistent_combinations'].append({\n",
    "                'task': task,\n",
    "                'dataset': dataset,\n",
    "                'model_name': model_name,\n",
    "                'inconsistent_prompting_ids': inconsistent_promptings\n",
    "            })\n",
    "    total_combinations = verification_results['consistent_combinations'] + len(verification_results['inconsistent_combinations'])\n",
    "    if verification_results['perfect_consistency']:\n",
    "        print(f\"   ✅ Verification: All {total_combinations} mapped groups have consistent demographics\")\n",
    "    else:\n",
    "        print(f\"   ❌ Verification: {len(verification_results['inconsistent_combinations'])}/{total_combinations} mapped groups have inconsistent demographics\")\n",
    "        for inc in verification_results['inconsistent_combinations']:\n",
    "            if 'inconsistent_models' in inc:\n",
    "                print(f\"     - {inc['task']}-{inc['dataset']}: {inc['inconsistent_models']}\")\n",
    "            else:\n",
    "                print(f\"     - {inc['task']}-{inc['dataset']} {inc['model_name']}: {inc['inconsistent_prompting_ids']}\")\n",
    "    return verification_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef6e551",
   "metadata": {},
   "source": [
    "### Running Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57378dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 68 metrics report files, 612 metadata files, 118 log files.\n",
      "Unpacked 1025336 batch rows into 1062044 individual sample rows\n",
      "Breakdown by model type:\n",
      "  LLM: 1,024,910 samples\n",
      "  convDL: 24,756 samples\n",
      "  convML: 12,378 samples\n",
      "Loaded metadata: 1062044 rows, 92 columns.\n",
      "\n",
      "🔢 Adding sample_index for consistent sample alignment...\n",
      "Sample indices added for all model-type groups (using original_row_index order).\n",
      "\n",
      "Loaded metadata: 1062044 records, columns: ['Sample ID', 'Step Name', 'Step Number', 'Target Label', 'metadata_patient_age', 'metadata_patient_sex', 'metadata_patient_weight', 'metadata_patient_height', 'metadata_monitoring_hours', 'metadata_total_features_available', 'metadata_data_completeness_score', 'metadata_imputation_percentage', 'metadata_agent_type', 'metadata_sample_start_time', 'System Message', 'Input Prompt', 'Output', 'Predicted Probability', 'Predicted Diagnosis', 'Predicted Explanation', 'Requested Tests', 'Confidence', 'Tokenization Time', 'Inference Time', 'Input Tokens', 'Output Tokens', 'metadata_ml_prediction', 'metadata_ml_confidence', 'metadata_ml_top_unique_features', 'metadata_xgb_model_available', 'metadata_xgb_feature_count', 'metadata_ml_vs_clinical_diff', 'metadata_ai_agreement', 'metadata_ml_confidence_adequate', 'metadata_investigation_triggered', 'metadata_ml_confidence_low', 'metadata_high_disagreement', 'metadata_agreement_threshold', 'metadata_objective_synthesis', 'metadata_dampened_clinical', 'metadata_max_allowed_deviation', 'metadata_dampening_applied', 'metadata_original_clinical', 'source_file', 'model_name', 'task', 'dataset', 'timestamp', 'prompting_id', 'is_agent', 'Thinking Tokens', 'Thinking Output', 'metadata_current_iteration', 'metadata_features_used_count', 'metadata_labs_requested_count', 'metadata_total_available_labs', 'metadata_total_unused_labs', 'metadata_stopping_reason', 'metadata_confidence_delta', 'metadata_probability_delta', 'metadata_confidence_improved', 'batch', 'prediction', 'label', 'loss', 'age', 'sex', 'height', 'weight', 'metadata_hemodynamic_features_available', 'metadata_metabolic_features_available', 'metadata_hematologic_features_available', 'metadata_hemodynamic_data_available', 'metadata_specialist_type', 'metadata_hemodynamic_confidence', 'metadata_hemodynamic_probability', 'metadata_metabolic_data_available', 'metadata_metabolic_confidence', 'metadata_metabolic_probability', 'metadata_hematologic_data_available', 'metadata_hematologic_confidence', 'metadata_hematologic_probability', 'metadata_specialist_probability_variance', 'metadata_specialist_probability_range', 'metadata_average_specialist_confidence', 'metadata_successful_specialists_count', 'metadata_failed_specialists_count', 'metadata_highest_prob_specialist', 'metadata_highest_conf_specialist', 'metadata_specialists_with_data', 'model_type', 'model_prompting_id', 'original_row_index', 'sample_index']\n",
      "Models: ['RandomForest', 'XGBoost', 'CNN', 'InceptionTime', 'GRU', 'LSTM', 'OpenAI-o3', 'Claude-Sonnet-4', 'Grok-4', 'Gemini-2.5-Pro', 'Gemini-2.5-Flash', 'Llama-3.1-8B-Instruct', 'Deepseek-R1-Distill-Llama-8B', 'Mistral-7B-Instruct-v0.3', 'Gemma-3-4B-it', 'MedGemma-4B-it']\n",
      "Prompting IDs: ['', 'Zero-Shot', 'HybReAgent', 'Aggregation', 'One-Shot', 'Few-Shot (3)', 'CoT', 'SumAgent', 'ColAgent', 'ClinFlowAgent']\n",
      "Tasks: ['Mortality', 'AKI', 'Sepsis']\n",
      "Datasets: ['HiRID', 'MIMIC-IV', 'eICU']\n",
      "\n",
      "--- Demographics Mapping Step ---\n",
      "   Processed 9 task-dataset combinations.\n",
      "   Updated 4 convDL models and 557 LLM model/prompting_id/task/dataset combinations.\n",
      "   Total samples updated: 407,484\n",
      "   Failed to map 1 model combinations.\n",
      "   ❌ Verification: 1/99 mapped groups have inconsistent demographics\n",
      "     - Sepsis-MIMIC-IV Grok-4: ['Zero-Shot']\n",
      "   Demographics mapping verification FAILED: 1 inconsistent combinations.\n",
      "--- End Demographics Mapping ---\n",
      "\n",
      "\n",
      "🔄 Adding probabilities column...\n",
      "   ✅ Converted logits to probabilities for convDL models: ['CNN', 'InceptionTime', 'GRU', 'LSTM']\n",
      "   ✅ Copied probabilities for convML models: ['RandomForest', 'XGBoost']\n",
      "   ✅ Copied probabilities for LLM models: ['OpenAI-o3', 'Claude-Sonnet-4', 'Grok-4', 'Gemini-2.5-Pro', 'Gemini-2.5-Flash', 'Llama-3.1-8B-Instruct', 'Deepseek-R1-Distill-Llama-8B', 'Mistral-7B-Instruct-v0.3', 'Gemma-3-4B-it', 'MedGemma-4B-it']\n",
      "   📊 Probabilities column added - range: [0.000, 1.000]\n",
      "Saved metadata to ./notebook_output/data/pulse_metadata.csv\n",
      "\n",
      "First 5 rows of metadata:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>model_name</th>\n",
       "      <th>is_agent</th>\n",
       "      <th>prompting_id</th>\n",
       "      <th>task</th>\n",
       "      <th>dataset</th>\n",
       "      <th>sample_index</th>\n",
       "      <th>original_row_index</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>...</th>\n",
       "      <th>metadata_specialist_probability_variance</th>\n",
       "      <th>metadata_specialist_probability_range</th>\n",
       "      <th>metadata_average_specialist_confidence</th>\n",
       "      <th>metadata_successful_specialists_count</th>\n",
       "      <th>metadata_failed_specialists_count</th>\n",
       "      <th>metadata_highest_prob_specialist</th>\n",
       "      <th>metadata_highest_conf_specialist</th>\n",
       "      <th>metadata_specialists_with_data</th>\n",
       "      <th>model_prompting_id</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>convML</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>Mortality</td>\n",
       "      <td>HiRID</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>/Users/sophiaehlers/Documents/pulse_agents/out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>convML</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>Mortality</td>\n",
       "      <td>HiRID</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>/Users/sophiaehlers/Documents/pulse_agents/out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>convML</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>Mortality</td>\n",
       "      <td>HiRID</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>/Users/sophiaehlers/Documents/pulse_agents/out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>convML</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>Mortality</td>\n",
       "      <td>HiRID</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>/Users/sophiaehlers/Documents/pulse_agents/out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>convML</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>Mortality</td>\n",
       "      <td>HiRID</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>/Users/sophiaehlers/Documents/pulse_agents/out...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_type    model_name  is_agent prompting_id       task dataset  \\\n",
       "0     convML  RandomForest     False               Mortality   HiRID   \n",
       "1     convML  RandomForest     False               Mortality   HiRID   \n",
       "2     convML  RandomForest     False               Mortality   HiRID   \n",
       "3     convML  RandomForest     False               Mortality   HiRID   \n",
       "4     convML  RandomForest     False               Mortality   HiRID   \n",
       "\n",
       "   sample_index  original_row_index  sex   age  ...  \\\n",
       "0             0                   0  1.0  55.0  ...   \n",
       "1             1                   1  0.0  50.0  ...   \n",
       "2             2                   2  0.0  30.0  ...   \n",
       "3             3                   3  0.0  60.0  ...   \n",
       "4             4                   4  1.0  75.0  ...   \n",
       "\n",
       "   metadata_specialist_probability_variance  \\\n",
       "0                                       NaN   \n",
       "1                                       NaN   \n",
       "2                                       NaN   \n",
       "3                                       NaN   \n",
       "4                                       NaN   \n",
       "\n",
       "   metadata_specialist_probability_range  \\\n",
       "0                                    NaN   \n",
       "1                                    NaN   \n",
       "2                                    NaN   \n",
       "3                                    NaN   \n",
       "4                                    NaN   \n",
       "\n",
       "   metadata_average_specialist_confidence  \\\n",
       "0                                     NaN   \n",
       "1                                     NaN   \n",
       "2                                     NaN   \n",
       "3                                     NaN   \n",
       "4                                     NaN   \n",
       "\n",
       "   metadata_successful_specialists_count  metadata_failed_specialists_count  \\\n",
       "0                                    NaN                                NaN   \n",
       "1                                    NaN                                NaN   \n",
       "2                                    NaN                                NaN   \n",
       "3                                    NaN                                NaN   \n",
       "4                                    NaN                                NaN   \n",
       "\n",
       "   metadata_highest_prob_specialist metadata_highest_conf_specialist  \\\n",
       "0                               NaN                              NaN   \n",
       "1                               NaN                              NaN   \n",
       "2                               NaN                              NaN   \n",
       "3                               NaN                              NaN   \n",
       "4                               NaN                              NaN   \n",
       "\n",
       "   metadata_specialists_with_data  model_prompting_id  \\\n",
       "0                             NaN        RandomForest   \n",
       "1                             NaN        RandomForest   \n",
       "2                             NaN        RandomForest   \n",
       "3                             NaN        RandomForest   \n",
       "4                             NaN        RandomForest   \n",
       "\n",
       "                                         source_file  \n",
       "0  /Users/sophiaehlers/Documents/pulse_agents/out...  \n",
       "1  /Users/sophiaehlers/Documents/pulse_agents/out...  \n",
       "2  /Users/sophiaehlers/Documents/pulse_agents/out...  \n",
       "3  /Users/sophiaehlers/Documents/pulse_agents/out...  \n",
       "4  /Users/sophiaehlers/Documents/pulse_agents/out...  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_and_save_metadata(outputfolder_path_list, save_path=\"./notebook_output/postprocessed_data/pulse_metadata.csv\"):\n",
    "    \"\"\"\n",
    "    Prepare df_metadata: load, add indices, map demographics, add probabilities, reorder columns, and save.\n",
    "    Returns the prepared DataFrame.\n",
    "    \"\"\"\n",
    "    cat_files = categorize_files(outputfolder_path_list)\n",
    "\n",
    "    # Load the metadata from identified files\n",
    "    df_metadata = load_metadata(cat_files[\"metadata_files\"])\n",
    "\n",
    "    # Add original_row_index before any sorting or mapping\n",
    "    df_metadata['original_row_index'] = np.arange(len(df_metadata))\n",
    "\n",
    "    # Add sample_index for consistent sample alignment across models\n",
    "    print(\"\\n🔢 Adding sample_index for consistent sample alignment...\")\n",
    "    df_metadata = add_sample_index_to_metadata(df_metadata)\n",
    "\n",
    "    # Display basic information about the loaded data\n",
    "    print(f\"\\nLoaded metadata: {len(df_metadata)} records, columns: {list(df_metadata.columns)}\")\n",
    "    print(f\"Models: {list(df_metadata['model_name'].unique())}\")\n",
    "    print(f\"Prompting IDs: {list(df_metadata['prompting_id'].unique())}\")\n",
    "    print(f\"Tasks: {list(df_metadata['task'].unique())}\")\n",
    "    print(f\"Datasets: {list(df_metadata['dataset'].unique())}\")\n",
    "\n",
    "    # Map convML demographics to all models for consistency (summary printed inside function)\n",
    "    df_metadata = map_convml_demographics_to_all_models(df_metadata)\n",
    "\n",
    "    # Add probabilities column (convert convDL logits to probabilities, copy convML and LLM probabilities)\n",
    "    print(\"\\n🔄 Adding probabilities column...\")\n",
    "    df_metadata['probabilities'] = df_metadata['prediction'].copy()\n",
    "\n",
    "    convdl_mask = df_metadata['model_type'] == 'convDL'\n",
    "    if convdl_mask.any():\n",
    "        df_metadata.loc[convdl_mask, 'probabilities'] = expit(df_metadata.loc[convdl_mask, 'prediction'])\n",
    "        convdl_models = df_metadata[convdl_mask]['model_name'].unique()\n",
    "        print(f\"   ✅ Converted logits to probabilities for convDL models: {list(convdl_models)}\")\n",
    "\n",
    "    convml_mask = df_metadata['model_type'] == 'convML'\n",
    "    if convml_mask.any():\n",
    "        convml_models = df_metadata[convml_mask]['model_name'].unique()\n",
    "        print(f\"   ✅ Copied probabilities for convML models: {list(convml_models)}\")\n",
    "\n",
    "    llm_mask = df_metadata['model_type'] == 'LLM'\n",
    "    if llm_mask.any():\n",
    "        llm_models = df_metadata[llm_mask]['model_name'].unique()\n",
    "        print(f\"   ✅ Copied probabilities for LLM models: {list(llm_models)}\")\n",
    "    print(f\"   📊 Probabilities column added - range: [{df_metadata['probabilities'].min():.3f}, {df_metadata['probabilities'].max():.3f}]\")\n",
    "\n",
    "    # Reorder columns to have key columns first\n",
    "    first_cols = [\n",
    "        'model_type', 'model_name', 'is_agent', 'prompting_id', 'task', 'dataset',\n",
    "        'sample_index', 'original_row_index', 'sex', 'age', 'height', 'weight',\n",
    "        'probabilities', 'prediction', 'label'\n",
    "    ]\n",
    "    first_cols_present = [col for col in first_cols if col in df_metadata.columns]\n",
    "    other_cols = [col for col in df_metadata.columns if col not in first_cols_present]\n",
    "    # Place source_file as the last column if it exists\n",
    "    if \"source_file\" in df_metadata.columns:\n",
    "        df_metadata = df_metadata[first_cols_present + other_cols + [\"source_file\"]]\n",
    "    else:\n",
    "        df_metadata = df_metadata[first_cols_present + other_cols]\n",
    "\n",
    "    # Save the prepared metadata DataFrame\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    df_metadata.to_csv(save_path, index=False)\n",
    "    print(f\"Saved metadata to {save_path}\")\n",
    "\n",
    "    # Show first few rows\n",
    "    print(\"\\nFirst 5 rows of metadata:\")\n",
    "    display(df_metadata.head())\n",
    "\n",
    "    return df_metadata\n",
    "\n",
    "# Prepare and save metadata\n",
    "df_metadata = prepare_and_save_metadata(outputfolder_path_list, save_path=\"./notebook_output/postprocessed_data/pulse_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "142a4d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 DEBUG: Checking task-dataset combination completeness for each model_prompting_id\n",
      "\n",
      "Expected 9 combinations: [('Mortality', 'HiRID'), ('Mortality', 'MIMIC-IV'), ('Mortality', 'eICU'), ('AKI', 'HiRID'), ('AKI', 'MIMIC-IV'), ('AKI', 'eICU'), ('Sepsis', 'HiRID'), ('Sepsis', 'MIMIC-IV'), ('Sepsis', 'eICU')]\n",
      "\n",
      "✅ CNN - All 9 combinations present\n",
      "✅ Claude-Sonnet-4, HybReAgent - All 9 combinations present\n",
      "✅ Claude-Sonnet-4, Zero-Shot - All 9 combinations present\n",
      "✅ Deepseek-R1-Distill-Llama-8B, Aggregation - All 9 combinations present\n",
      "✅ Deepseek-R1-Distill-Llama-8B, ClinFlowAgent - All 9 combinations present\n",
      "✅ Deepseek-R1-Distill-Llama-8B, CoT - All 9 combinations present\n",
      "✅ Deepseek-R1-Distill-Llama-8B, ColAgent - All 9 combinations present\n",
      "✅ Deepseek-R1-Distill-Llama-8B, Few-Shot (3) - All 9 combinations present\n",
      "✅ Deepseek-R1-Distill-Llama-8B, HybReAgent - All 9 combinations present\n",
      "✅ Deepseek-R1-Distill-Llama-8B, One-Shot - All 9 combinations present\n",
      "✅ Deepseek-R1-Distill-Llama-8B, SumAgent - All 9 combinations present\n",
      "✅ Deepseek-R1-Distill-Llama-8B, Zero-Shot - All 9 combinations present\n",
      "✅ GRU - All 9 combinations present\n",
      "✅ Gemini-2.5-Flash, Aggregation - All 9 combinations present\n",
      "✅ Gemini-2.5-Flash, ClinFlowAgent - All 9 combinations present\n",
      "✅ Gemini-2.5-Flash, CoT - All 9 combinations present\n",
      "✅ Gemini-2.5-Flash, ColAgent - All 9 combinations present\n",
      "✅ Gemini-2.5-Flash, Few-Shot (3) - All 9 combinations present\n",
      "✅ Gemini-2.5-Flash, HybReAgent - All 9 combinations present\n",
      "✅ Gemini-2.5-Flash, One-Shot - All 9 combinations present\n",
      "✅ Gemini-2.5-Flash, SumAgent - All 9 combinations present\n",
      "✅ Gemini-2.5-Flash, Zero-Shot - All 9 combinations present\n",
      "✅ Gemini-2.5-Pro, HybReAgent - All 9 combinations present\n",
      "✅ Gemini-2.5-Pro, Zero-Shot - All 9 combinations present\n",
      "✅ Gemma-3-4B-it, Aggregation - All 9 combinations present\n",
      "✅ Gemma-3-4B-it, ClinFlowAgent - All 9 combinations present\n",
      "✅ Gemma-3-4B-it, CoT - All 9 combinations present\n",
      "✅ Gemma-3-4B-it, ColAgent - All 9 combinations present\n",
      "✅ Gemma-3-4B-it, Few-Shot (3) - All 9 combinations present\n",
      "✅ Gemma-3-4B-it, HybReAgent - All 9 combinations present\n",
      "✅ Gemma-3-4B-it, One-Shot - All 9 combinations present\n",
      "✅ Gemma-3-4B-it, SumAgent - All 9 combinations present\n",
      "✅ Gemma-3-4B-it, Zero-Shot - All 9 combinations present\n",
      "✅ Grok-4, HybReAgent - All 9 combinations present\n",
      "✅ Grok-4, Zero-Shot - All 9 combinations present\n",
      "✅ InceptionTime - All 9 combinations present\n",
      "✅ LSTM - All 9 combinations present\n",
      "✅ Llama-3.1-8B-Instruct, Aggregation - All 9 combinations present\n",
      "✅ Llama-3.1-8B-Instruct, ClinFlowAgent - All 9 combinations present\n",
      "✅ Llama-3.1-8B-Instruct, CoT - All 9 combinations present\n",
      "✅ Llama-3.1-8B-Instruct, ColAgent - All 9 combinations present\n",
      "✅ Llama-3.1-8B-Instruct, Few-Shot (3) - All 9 combinations present\n",
      "✅ Llama-3.1-8B-Instruct, HybReAgent - All 9 combinations present\n",
      "✅ Llama-3.1-8B-Instruct, One-Shot - All 9 combinations present\n",
      "✅ Llama-3.1-8B-Instruct, SumAgent - All 9 combinations present\n",
      "✅ Llama-3.1-8B-Instruct, Zero-Shot - All 9 combinations present\n",
      "✅ MedGemma-4B-it, Aggregation - All 9 combinations present\n",
      "✅ MedGemma-4B-it, ClinFlowAgent - All 9 combinations present\n",
      "✅ MedGemma-4B-it, CoT - All 9 combinations present\n",
      "✅ MedGemma-4B-it, ColAgent - All 9 combinations present\n",
      "✅ MedGemma-4B-it, Few-Shot (3) - All 9 combinations present\n",
      "✅ MedGemma-4B-it, HybReAgent - All 9 combinations present\n",
      "✅ MedGemma-4B-it, One-Shot - All 9 combinations present\n",
      "✅ MedGemma-4B-it, SumAgent - All 9 combinations present\n",
      "✅ MedGemma-4B-it, Zero-Shot - All 9 combinations present\n",
      "✅ Mistral-7B-Instruct-v0.3, Aggregation - All 9 combinations present\n",
      "✅ Mistral-7B-Instruct-v0.3, ClinFlowAgent - All 9 combinations present\n",
      "✅ Mistral-7B-Instruct-v0.3, CoT - All 9 combinations present\n",
      "✅ Mistral-7B-Instruct-v0.3, ColAgent - All 9 combinations present\n",
      "✅ Mistral-7B-Instruct-v0.3, Few-Shot (3) - All 9 combinations present\n",
      "✅ Mistral-7B-Instruct-v0.3, HybReAgent - All 9 combinations present\n",
      "✅ Mistral-7B-Instruct-v0.3, One-Shot - All 9 combinations present\n",
      "✅ Mistral-7B-Instruct-v0.3, SumAgent - All 9 combinations present\n",
      "✅ Mistral-7B-Instruct-v0.3, Zero-Shot - All 9 combinations present\n",
      "✅ OpenAI-o3, HybReAgent - All 9 combinations present\n",
      "✅ OpenAI-o3, Zero-Shot - All 9 combinations present\n",
      "✅ RandomForest - All 9 combinations present\n",
      "✅ XGBoost - All 9 combinations present\n",
      "\n",
      "📊 SUMMARY:\n",
      "🎉 All 68 model_prompting_ids have complete task-dataset combinations!\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check if each model_prompting_id has all 9 task-dataset combinations\n",
    "print(\"🔍 DEBUG: Checking task-dataset combination completeness for each model_prompting_id\\n\")\n",
    "\n",
    "# Define expected combinations\n",
    "expected_tasks = list(TASK_MAPPING.values())  # [\"Mortality\", \"AKI\", \"Sepsis\"]\n",
    "expected_datasets = list(DATASET_MAPPING.values())  # [\"HiRID\", \"MIMIC-IV\", \"eICU\"]\n",
    "expected_combinations = [(task, dataset) for task in expected_tasks for dataset in expected_datasets]\n",
    "print(f\"Expected {len(expected_combinations)} combinations: {expected_combinations}\\n\")\n",
    "\n",
    "# Get actual combinations for each model_prompting_id\n",
    "model_prompting_ids = df_metadata['model_prompting_id'].unique()\n",
    "all_complete = True\n",
    "missing_summary = []\n",
    "\n",
    "for model_prompting_id in sorted(model_prompting_ids):\n",
    "    model_data = df_metadata[df_metadata['model_prompting_id'] == model_prompting_id]\n",
    "    actual_combinations = set(zip(model_data['task'], model_data['dataset']))\n",
    "    expected_combinations_set = set(expected_combinations)\n",
    "    \n",
    "    missing_combinations = expected_combinations_set - actual_combinations\n",
    "    \n",
    "    if missing_combinations:\n",
    "        all_complete = False\n",
    "        missing_list = sorted(list(missing_combinations))\n",
    "        missing_summary.append((model_prompting_id, missing_list))\n",
    "        print(f\"❌ {model_prompting_id}\")\n",
    "        print(f\"   Missing {len(missing_combinations)}/{len(expected_combinations)} combinations:\")\n",
    "        for task, dataset in missing_list:\n",
    "            print(f\"   - {task} + {dataset}\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"✅ {model_prompting_id} - All {len(expected_combinations)} combinations present\")\n",
    "\n",
    "print(f\"\\n📊 SUMMARY:\")\n",
    "if all_complete:\n",
    "    print(f\"🎉 All {len(model_prompting_ids)} model_prompting_ids have complete task-dataset combinations!\")\n",
    "else:\n",
    "    complete_count = len(model_prompting_ids) - len(missing_summary)\n",
    "    print(f\"⚠️  {complete_count}/{len(model_prompting_ids)} model_prompting_ids are complete\")\n",
    "    print(f\"   {len(missing_summary)} model_prompting_ids have missing combinations:\")\n",
    "    for model_prompting_id, missing_list in missing_summary:\n",
    "        print(f\"   - {model_prompting_id}: {len(missing_list)} missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22b3a02",
   "metadata": {},
   "source": [
    "## Metrics Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421aaaa1",
   "metadata": {},
   "source": [
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c21e91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r1/8n2p36gx2ydfw7wf826zxyh00000gn/T/ipykernel_46771/667825469.py:18: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  for group_keys, group in df_metadata.groupby(group_cols):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Summary:\n",
      "   Total metric combinations: 5704\n",
      "Saved metrics to ./notebook_output/data/pulse_metrics.csv\n",
      "📈 Summary:\n",
      "   Total metric combinations: 5704\n",
      "📊 SUBGROUP DEFINITIONS:\n",
      "   Age: ['18-65 Years', '65-75 Years', '75-91 Years']\n",
      "   BMI: ['BMI 18.5-25 kg/m2', 'BMI 25-30 kg/m2', 'BMI < 18.5 kg/m2', 'BMI > 30 kg/m2']\n",
      "   Overall: ['All']\n",
      "   Sex: ['Female', 'Male']\n",
      "\n",
      "✅ All subgroups have positive labels\n",
      "\n",
      "📋 FIRST 10 ROWS OF df_metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>Model</th>\n",
       "      <th>Task</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>subgroup_type</th>\n",
       "      <th>subgroup_value</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>AUPRC</th>\n",
       "      <th>Normalized AUPRC</th>\n",
       "      <th>Min(+P, Se)</th>\n",
       "      <th>...</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>MCC</th>\n",
       "      <th>Cohen's Kappa</th>\n",
       "      <th>is_agent</th>\n",
       "      <th>prompting_id</th>\n",
       "      <th>sample_count</th>\n",
       "      <th>positive_count</th>\n",
       "      <th>negative_count</th>\n",
       "      <th>positive_rate</th>\n",
       "      <th>model_prompting_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>convML</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>Mortality</td>\n",
       "      <td>HiRID</td>\n",
       "      <td>Overall</td>\n",
       "      <td>All</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.614</td>\n",
       "      <td>5.580</td>\n",
       "      <td>0.500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.283</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>100</td>\n",
       "      <td>11</td>\n",
       "      <td>89</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>RandomForest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>convML</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>Mortality</td>\n",
       "      <td>HiRID</td>\n",
       "      <td>Sex</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.700</td>\n",
       "      <td>5.162</td>\n",
       "      <td>0.545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.366</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>59</td>\n",
       "      <td>8</td>\n",
       "      <td>51</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>RandomForest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>convML</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>Mortality</td>\n",
       "      <td>HiRID</td>\n",
       "      <td>Sex</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.917</td>\n",
       "      <td>0.285</td>\n",
       "      <td>3.891</td>\n",
       "      <td>0.375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>RandomForest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>convML</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>Mortality</td>\n",
       "      <td>HiRID</td>\n",
       "      <td>Age</td>\n",
       "      <td>18-65 Years</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.817</td>\n",
       "      <td>9.528</td>\n",
       "      <td>0.667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.478</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>RandomForest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>convML</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>Mortality</td>\n",
       "      <td>HiRID</td>\n",
       "      <td>Age</td>\n",
       "      <td>65-75 Years</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.559</td>\n",
       "      <td>5.593</td>\n",
       "      <td>0.500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>RandomForest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>convML</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>Mortality</td>\n",
       "      <td>HiRID</td>\n",
       "      <td>Age</td>\n",
       "      <td>75-91 Years</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.518</td>\n",
       "      <td>3.628</td>\n",
       "      <td>0.429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.300</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>RandomForest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>convML</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>Mortality</td>\n",
       "      <td>HiRID</td>\n",
       "      <td>BMI</td>\n",
       "      <td>BMI 18.5-25 kg/m2</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.682</td>\n",
       "      <td>6.311</td>\n",
       "      <td>0.500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.373</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>RandomForest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>convML</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>Mortality</td>\n",
       "      <td>HiRID</td>\n",
       "      <td>BMI</td>\n",
       "      <td>BMI 25-30 kg/m2</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.515</td>\n",
       "      <td>4.018</td>\n",
       "      <td>0.500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>RandomForest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>convML</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>Mortality</td>\n",
       "      <td>HiRID</td>\n",
       "      <td>BMI</td>\n",
       "      <td>BMI &gt; 30 kg/m2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>9.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.640</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>RandomForest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>convML</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>Mortality</td>\n",
       "      <td>MIMIC-IV</td>\n",
       "      <td>Overall</td>\n",
       "      <td>All</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.652</td>\n",
       "      <td>5.434</td>\n",
       "      <td>0.667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>100</td>\n",
       "      <td>12</td>\n",
       "      <td>88</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>RandomForest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_type         Model       Task   Dataset subgroup_type  \\\n",
       "0     convML  RandomForest  Mortality     HiRID       Overall   \n",
       "1     convML  RandomForest  Mortality     HiRID           Sex   \n",
       "2     convML  RandomForest  Mortality     HiRID           Sex   \n",
       "3     convML  RandomForest  Mortality     HiRID           Age   \n",
       "4     convML  RandomForest  Mortality     HiRID           Age   \n",
       "5     convML  RandomForest  Mortality     HiRID           Age   \n",
       "6     convML  RandomForest  Mortality     HiRID           BMI   \n",
       "7     convML  RandomForest  Mortality     HiRID           BMI   \n",
       "8     convML  RandomForest  Mortality     HiRID           BMI   \n",
       "9     convML  RandomForest  Mortality  MIMIC-IV       Overall   \n",
       "\n",
       "      subgroup_value  AUROC  AUPRC  Normalized AUPRC  Min(+P, Se)  ...  \\\n",
       "0                All  0.902  0.614             5.580        0.500  ...   \n",
       "1               Male  0.888  0.700             5.162        0.545  ...   \n",
       "2             Female  0.917  0.285             3.891        0.375  ...   \n",
       "3        18-65 Years  0.969  0.817             9.528        0.667  ...   \n",
       "4        65-75 Years  0.889  0.559             5.593        0.500  ...   \n",
       "5        75-91 Years  0.863  0.518             3.628        0.429  ...   \n",
       "6  BMI 18.5-25 kg/m2  0.909  0.682             6.311        0.500  ...   \n",
       "7    BMI 25-30 kg/m2  0.885  0.515             4.018        0.500  ...   \n",
       "8     BMI > 30 kg/m2  1.000  1.000             9.000        1.000  ...   \n",
       "9                All  0.823  0.652             5.434        0.667  ...   \n",
       "\n",
       "   Balanced Accuracy    MCC  Cohen's Kappa  is_agent  prompting_id  \\\n",
       "0              0.591  0.406          0.283     False                 \n",
       "1              0.625  0.473          0.366     False                 \n",
       "2              0.500  0.000          0.000     False                 \n",
       "3              0.667  0.560          0.478     False                 \n",
       "4              0.500  0.000          0.000     False                 \n",
       "5              0.600  0.420          0.300     False                 \n",
       "6              0.625  0.479          0.373     False                 \n",
       "7              0.500  0.000          0.000     False                 \n",
       "8              0.750  0.686          0.640     False                 \n",
       "9              0.500  0.000          0.000     False                 \n",
       "\n",
       "   sample_count  positive_count  negative_count  positive_rate  \\\n",
       "0           100              11              89       0.110000   \n",
       "1            59               8              51       0.135593   \n",
       "2            41               3              38       0.073171   \n",
       "3            35               3              32       0.085714   \n",
       "4            30               3              27       0.100000   \n",
       "5            35               5              30       0.142857   \n",
       "6            37               4              33       0.108108   \n",
       "7            39               5              34       0.128205   \n",
       "8            18               2              16       0.111111   \n",
       "9           100              12              88       0.120000   \n",
       "\n",
       "  model_prompting_id  \n",
       "0       RandomForest  \n",
       "1       RandomForest  \n",
       "2       RandomForest  \n",
       "3       RandomForest  \n",
       "4       RandomForest  \n",
       "5       RandomForest  \n",
       "6       RandomForest  \n",
       "7       RandomForest  \n",
       "8       RandomForest  \n",
       "9       RandomForest  \n",
       "\n",
       "[10 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calculate_subgroup_metrics(df_metadata):\n",
    "    \"\"\"\n",
    "    Calculate subgroup metrics for all models, tasks, datasets and subgroups.\n",
    "    Also includes overall metrics (without subgroups) as the first row for each combination.\n",
    "    Adds 'prompting_id' and 'is_agent' columns to the metrics DataFrame.\n",
    "    Args:\n",
    "        df_metadata (DataFrame): Metadata DataFrame with predictions and labels\n",
    "    Returns:\n",
    "        DataFrame: Single DataFrame with overall and all subgroup metrics\n",
    "    \"\"\"\n",
    "    df_metadata = add_bmi_to_metadata(df_metadata)\n",
    "    all_metrics_list = []\n",
    "    # Group by model_type, model_name, is_agent, prompting_id, task, dataset (in this order, only if columns exist)\n",
    "    group_cols = []\n",
    "    for col in ['model_type', 'model_name', 'is_agent', 'prompting_id', 'task', 'dataset']:\n",
    "        if col in df_metadata.columns:\n",
    "            group_cols.append(col)\n",
    "    for group_keys, group in df_metadata.groupby(group_cols):\n",
    "        # Unpack group keys in the correct order\n",
    "        key_dict = dict(zip(group_cols, group_keys if isinstance(group_keys, tuple) else [group_keys]))\n",
    "        model = key_dict.get('model_name', '')\n",
    "        is_agent = key_dict.get('is_agent', False)\n",
    "        prompting_id = key_dict.get('prompting_id', '')\n",
    "        task = key_dict.get('task', '')\n",
    "        dataset = key_dict.get('dataset', '')\n",
    "        # Filter for agent: only Step Name == 'final_prediction'\n",
    "        group_for_metrics = group\n",
    "        if is_agent and 'Step Name' in group.columns:\n",
    "            group_for_metrics = group[group['Step Name'] == 'final_prediction']\n",
    "        overall_metrics = calculate_overall_metrics_helper(group_for_metrics, model, task, dataset)\n",
    "        if overall_metrics:\n",
    "            overall_metrics['prompting_id'] = prompting_id\n",
    "            overall_metrics['is_agent'] = is_agent\n",
    "            all_metrics_list.append(overall_metrics)\n",
    "        all_subgroups = calculate_all_subgroup_metrics(group_for_metrics, model, task, dataset)\n",
    "        for m in all_subgroups:\n",
    "            m['prompting_id'] = prompting_id\n",
    "            m['is_agent'] = is_agent\n",
    "        all_metrics_list.extend(all_subgroups)\n",
    "    df_metrics = pd.DataFrame(all_metrics_list)\n",
    "    print(f\"📈 Summary:\")\n",
    "    print(f\"   Total metric combinations: {len(df_metrics)}\")\n",
    "    return df_metrics\n",
    "\n",
    "\n",
    "def calculate_overall_metrics_helper(group_data, model, task, dataset):\n",
    "    \"\"\"Helper function to calculate overall metrics for the entire group.\"\"\"\n",
    "    try:\n",
    "        predictions = group_data['probabilities'].values  # Use probabilities column\n",
    "        labels = group_data['label'].values\n",
    "\n",
    "        # Calculate basic metrics\n",
    "        metrics = calculate_all_metrics(labels, predictions)\n",
    "\n",
    "        # Add metadata and counts, with key columns first\n",
    "        metrics_ordered = {\n",
    "            \"model_name\": model,\n",
    "            \"is_agent\": (\n",
    "                group_data[\"is_agent\"].iloc[0]\n",
    "                if \"is_agent\" in group_data.columns\n",
    "                else False\n",
    "            ),\n",
    "            \"prompting_id\": (\n",
    "                group_data[\"prompting_id\"].iloc[0]\n",
    "                if \"prompting_id\" in group_data.columns\n",
    "                else \"\"\n",
    "            ),\n",
    "            \"task\": task,\n",
    "            \"dataset\": dataset,\n",
    "            \"subgroup_type\": \"Overall\",\n",
    "            \"subgroup_value\": \"All\",\n",
    "            \"sample_count\": len(predictions),\n",
    "            \"positive_count\": int(np.sum(labels)),\n",
    "            \"negative_count\": int(len(labels) - np.sum(labels)),\n",
    "            \"positive_rate\": np.mean(labels),\n",
    "        }\n",
    "        metrics_ordered.update(metrics)\n",
    "        metrics = metrics_ordered\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "def add_bmi_to_metadata(df_metadata):\n",
    "    \"\"\"\n",
    "    Add BMI calculation and categorization to the metadata DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df_metadata (DataFrame): Metadata DataFrame with height and weight\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with BMI and BMI_category columns added\n",
    "    \"\"\"\n",
    "    df_result = df_metadata.copy()\n",
    "\n",
    "    # Calculate BMI: weight (kg) / height (m)^2\n",
    "    # Height is in cm, convert to meters\n",
    "    height_m = df_result['height'] / 100\n",
    "    df_result['BMI'] = df_result['weight'] / (height_m ** 2)\n",
    "\n",
    "    # Categorize BMI according to WHO standards\n",
    "    def categorize_bmi(bmi):\n",
    "        if pd.isna(bmi):\n",
    "            return 'Unknown'\n",
    "        elif bmi < 18.5:\n",
    "            return 'BMI < 18.5 kg/m2'\n",
    "        elif bmi < 25.0:\n",
    "            return 'BMI 18.5-25 kg/m2'\n",
    "        elif bmi < 30.0:\n",
    "            return 'BMI 25-30 kg/m2'\n",
    "        else:\n",
    "            return 'BMI > 30 kg/m2'\n",
    "\n",
    "    df_result['bmi_category'] = df_result['BMI'].apply(categorize_bmi)\n",
    "\n",
    "    # Order BMI categories correctly\n",
    "    df_result[\"bmi_category\"] = pd.Categorical(\n",
    "        df_result[\"bmi_category\"], categories=BMI_LIST, ordered=True\n",
    "    )\n",
    "\n",
    "    return df_result\n",
    "\n",
    "def calculate_all_subgroup_metrics(group_data, model, task, dataset):\n",
    "    \"\"\"Calculate metrics for all subgroups (sex, age, BMI) in a single function.\"\"\"\n",
    "    all_subgroups = []\n",
    "    \n",
    "    # 1. Sex subgroups\n",
    "    sex_groups = {\n",
    "        'Male': group_data[group_data['sex'] == 1],\n",
    "        'Female': group_data[group_data['sex'] == 0]\n",
    "    }\n",
    "    \n",
    "    for sex, sex_group in sex_groups.items():\n",
    "        if len(sex_group) > 0 and len(sex_group['label'].unique()) >= 2:\n",
    "            metrics = calculate_subgroup_metrics_helper(sex_group, model, task, dataset, 'Sex', sex)\n",
    "            if metrics:\n",
    "                all_subgroups.append(metrics)\n",
    "    \n",
    "    # 2. Age subgroups\n",
    "    age_groups = {\n",
    "        '18-65 Years': group_data[(group_data['age'] >= 18) & (group_data['age'] < 65)],\n",
    "        '65-75 Years': group_data[(group_data['age'] >= 65) & (group_data['age'] < 75)],\n",
    "        '75-91 Years': group_data[(group_data['age'] >= 75) & (group_data['age'] <= 91)]\n",
    "    }\n",
    "    \n",
    "    for age_range, age_group in age_groups.items():\n",
    "        if len(age_group) > 0 and len(age_group['label'].unique()) >= 2:\n",
    "            metrics = calculate_subgroup_metrics_helper(age_group, model, task, dataset, 'Age', age_range)\n",
    "            if metrics:\n",
    "                all_subgroups.append(metrics)\n",
    "    \n",
    "    # 3. BMI subgroups\n",
    "    bmi_categories = group_data['bmi_category'].unique()\n",
    "    bmi_categories = [cat for cat in BMI_LIST if cat != 'Unknown']\n",
    "    \n",
    "    for bmi_category in bmi_categories:\n",
    "        bmi_group = group_data[group_data['bmi_category'] == bmi_category]\n",
    "        if len(bmi_group) > 0 and len(bmi_group['label'].unique()) >= 2:\n",
    "            metrics = calculate_subgroup_metrics_helper(bmi_group, model, task, dataset, 'BMI', bmi_category)\n",
    "            if metrics:\n",
    "                all_subgroups.append(metrics)\n",
    "    \n",
    "    return all_subgroups\n",
    "\n",
    "\n",
    "def calculate_subgroup_metrics_helper(subgroup_data, model, task, dataset, subgroup_type, subgroup_value):\n",
    "    \"\"\"Helper function to calculate metrics for a single subgroup.\"\"\"\n",
    "    try:\n",
    "        predictions = subgroup_data['probabilities'].values  # Use probabilities column\n",
    "        labels = subgroup_data['label'].values\n",
    "        \n",
    "        # Calculate basic metrics\n",
    "        metrics = calculate_all_metrics(labels, predictions)\n",
    "        \n",
    "        # Add metadata and counts\n",
    "        metrics.update({\n",
    "            'model_name': model,\n",
    "            'task': task,\n",
    "            'dataset': dataset,\n",
    "            'subgroup_type': subgroup_type,\n",
    "            'subgroup_value': subgroup_value,\n",
    "            'sample_count': len(predictions),\n",
    "            'positive_count': int(np.sum(labels)),\n",
    "            'negative_count': int(len(labels) - np.sum(labels)),\n",
    "            'positive_rate': np.mean(labels)\n",
    "        })\n",
    "        \n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "def display_metrics_summary(df_metrics):\n",
    "    \"\"\"Display a simplified summary of calculated metrics.\"\"\"\n",
    "    print(f\"📈 Summary:\")\n",
    "    print(f\"   Total metric combinations: {len(df_metrics)}\")\n",
    "\n",
    "    print(\"📊 SUBGROUP DEFINITIONS:\")\n",
    "    for subgroup_type in sorted(df_metrics['subgroup_type'].unique()):\n",
    "        values = sorted(df_metrics[df_metrics['subgroup_type'] == subgroup_type]['subgroup_value'].unique())\n",
    "        print(f\"   {subgroup_type}: {values}\")\n",
    "\n",
    "    # Check for subgroups with no positive labels\n",
    "    zero_positive = df_metrics[df_metrics['positive_count'] == 0]\n",
    "    if not zero_positive.empty:\n",
    "        print(f\"\\n⚠️  SUBGROUPS WITH NO POSITIVE LABELS ({len(zero_positive)} found):\")\n",
    "        for _, row in zero_positive.iterrows():\n",
    "            print(f\"   {row['model_name']} - {row['task']} - {row['dataset']} - {row['subgroup_type']}:{row['subgroup_value']}\")\n",
    "    else:\n",
    "        print(f\"\\n✅ All subgroups have positive labels\")\n",
    "\n",
    "    print(f\"\\n📋 FIRST 10 ROWS OF df_metrics:\")\n",
    "\n",
    "# Calculate metrics (overall + subgroups)\n",
    "df_metrics = calculate_subgroup_metrics(df_metadata)\n",
    "\n",
    "# Add model_type to metrics DataFrame (copied from metadata)\n",
    "if 'model_name' in df_metrics.columns and 'model_name' in df_metadata.columns:\n",
    "    model_type_map = dict(zip(df_metadata['model_name'], df_metadata['model_type']))\n",
    "    df_metrics['model_type'] = df_metrics['model_name'].map(model_type_map)\n",
    "\n",
    "# Add model_prompting_id column to metrics DataFrame\n",
    "if 'model_name' in df_metrics.columns and 'model_type' in df_metrics.columns:\n",
    "    def get_model_prompting_id_metrics(row):\n",
    "        if row.get('model_type') == 'LLM' and row.get('prompting_id', ''):\n",
    "            return f\"{row['model_name']}, {row['prompting_id']}\"\n",
    "        else:\n",
    "            return row['model_name']\n",
    "    df_metrics['model_prompting_id'] = df_metrics.apply(get_model_prompting_id_metrics, axis=1)\n",
    "\n",
    "# Reorder columns to have model_type first\n",
    "cols = list(df_metrics.columns)\n",
    "if \"model_type\" in cols:\n",
    "    cols = [\"model_type\"] + [c for c in cols if c != \"model_type\"]\n",
    "    df_metrics = df_metrics[cols]\n",
    "\n",
    "# Rename columns\n",
    "df_metrics = df_metrics.rename(\n",
    "    columns={\n",
    "        **METRICS_MAPPING,\n",
    "        'model_name': 'Model',\n",
    "        'task': 'Task',\n",
    "        'dataset': 'Dataset'\n",
    "    }\n",
    ")\n",
    "\n",
    "# After renaming columns\n",
    "metric_cols = [\n",
    "    \"AUROC\", \"AUPRC\", \"Normalized AUPRC\", \"Min(+P, Se)\", \"Sensitivity (Recall)\",\n",
    "    \"Specificity\", \"Precision\", \"F1 Score\", \"Accuracy\", \"Balanced Accuracy\", \"MCC\", \"Cohen's Kappa\"\n",
    "]\n",
    "# Place these after your key columns (e.g., Model, Task, Dataset, etc.)\n",
    "key_cols = [\"model_type\", \"Model\", \"Task\", \"Dataset\", \"subgroup_type\", \"subgroup_value\"]\n",
    "df_metrics = df_metrics[key_cols + metric_cols + [c for c in df_metrics.columns if c not in key_cols + metric_cols]]\n",
    "\n",
    "# Save df_metrics as pulse_metrics.csv in ./notebook_output/data\n",
    "os.makedirs(\"./notebook_output/postprocessed_data\", exist_ok=True)\n",
    "df_metrics.to_csv(\"./notebook_output/postprocessed_data/pulse_metrics.csv\", index=False)\n",
    "print(\"Saved metrics to ./notebook_output/postprocessed_data/pulse_metrics.csv\")\n",
    "\n",
    "# Display summary\n",
    "display_metrics_summary(df_metrics)\n",
    "display(df_metrics.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93ff00e",
   "metadata": {},
   "source": [
    "### Save Performance Metrics in CSV Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "031b3047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved baseline metrics table to ./notebook_output/metrics_tables/baseline_all_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Save baseline model metrics in publication-ready format ---\n",
    "def save_baseline_metrics_table(df_metrics, save_path=\"./notebook_output/metrics_tables/baseline_metrics.csv\"):\n",
    "    \"\"\"\n",
    "    Save baseline model metrics in a publication-ready format with hierarchical rows (see screenshot).\n",
    "    Only includes convML and convDL models, and only overall metrics (not subgroups).\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    # Filter for convML and convDL, and only overall metrics\n",
    "    df = df_metrics[\n",
    "        (df_metrics[\"model_type\"].isin([\"convML\", \"convDL\"])) &\n",
    "        (df_metrics[\"subgroup_type\"] == \"Overall\")\n",
    "    ].copy()\n",
    "\n",
    "    # Define the order for model types, models, tasks, datasets\n",
    "    model_type_order = [\"convML\", \"convDL\"]\n",
    "    convml_model_order = [\"RandomForest\", \"XGBoost\"]\n",
    "    convdl_model_order = [\"CNN\", \"InceptionTime\", \"LSTM\", \"GRU\"]\n",
    "    model_order = convml_model_order + convdl_model_order\n",
    "    task_order = [\"Mortality\", \"AKI\", \"Sepsis\"]\n",
    "    dataset_order = [\"HiRID\", \"MIMIC-IV\", \"eICU\"]\n",
    "\n",
    "    # Prepare columns for output\n",
    "    metric_cols = [\n",
    "        \"AUROC\", \"AUPRC\", \"Normalized AUPRC\", \"Min(+P, Se)\", \"Sensitivity (Recall)\", \"Specificity\",\n",
    "        \"Precision\", \"F1 Score\", \"Accuracy\", \"Balanced Accuracy\", \"MCC\", \"Cohen's Kappa\"\n",
    "    ]\n",
    "    # Ensure all metric columns exist\n",
    "    for col in metric_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "\n",
    "    # Build the hierarchical table\n",
    "    rows = []\n",
    "    for model_type in model_type_order:\n",
    "        models = convml_model_order if model_type == \"convML\" else convdl_model_order\n",
    "        for model in models:\n",
    "            for task in task_order:\n",
    "                for dataset in dataset_order:\n",
    "                    # Find the row in df\n",
    "                    row = df[\n",
    "                        (df[\"model_type\"] == model_type) &\n",
    "                        (df[\"Model\"] == model) &\n",
    "                        (df[\"Task\"] == task) &\n",
    "                        (df[\"Dataset\"] == dataset)\n",
    "                    ]\n",
    "                    # Prepare row values\n",
    "                    row_dict = {\n",
    "                        \"Model Type\": model_type,\n",
    "                        \"Model\": model,\n",
    "                        \"Task\": task,\n",
    "                        \"Dataset\": dataset\n",
    "                    }\n",
    "                    if not row.empty:\n",
    "                        for col in metric_cols:\n",
    "                            row_dict[col] = row.iloc[0][col]\n",
    "                    else:\n",
    "                        for col in metric_cols:\n",
    "                            row_dict[col] = \"\"\n",
    "                    rows.append(row_dict)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_out = pd.DataFrame(rows)\n",
    "\n",
    "    # Replace repeated values with empty string for hierarchical effect\n",
    "    for col in [\"Model Type\", \"Model\", \"Task\"]:\n",
    "        last_val = None\n",
    "        for i in range(len(df_out)):\n",
    "            if df_out.loc[i, col] == last_val:\n",
    "                df_out.loc[i, col] = \"\"\n",
    "            else:\n",
    "                last_val = df_out.loc[i, col]\n",
    "\n",
    "    # Save to CSV\n",
    "    df_out.to_csv(save_path, index=False)\n",
    "    print(f\"Saved baseline metrics table to {save_path}\")\n",
    "\n",
    "# Usage:\n",
    "save_baseline_metrics_table(\n",
    "    df_metrics,\n",
    "    save_path=\"./notebook_output/metrics_tables/baseline_all_metrics.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2fcdcaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LLM metrics table (is_agent=None) to ./notebook_output/metrics_tables/llm_all_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "def save_llm_metrics_table(\n",
    "    df_metrics,\n",
    "    is_agent=None,\n",
    "    save_path=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Save LLM model metrics in publication-ready format with hierarchical rows.\n",
    "    If is_agent is None, save all LLM metrics (both agent and non-agent) into one file.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    # Filter for LLMs, only overall metrics\n",
    "    df = df_metrics[\n",
    "        (df_metrics[\"model_type\"] == \"LLM\")\n",
    "        & (df_metrics[\"subgroup_type\"] == \"Overall\")\n",
    "    ].copy()\n",
    "\n",
    "    # Filter by is_agent if not None\n",
    "    if is_agent is not None:\n",
    "        df = df[df[\"is_agent\"] == is_agent]\n",
    "\n",
    "    # Define the order for models, prompting methods, tasks, datasets\n",
    "    if is_agent is None:\n",
    "        prompting_order = list(PROMPTING_ID_DISPLAY_MAPPING.values())\n",
    "    elif is_agent:\n",
    "        prompting_order = [\n",
    "            v for k, v in PROMPTING_ID_DISPLAY_MAPPING.items() if \"agent\" in k.lower()\n",
    "        ]\n",
    "    else:\n",
    "        prompting_order = [\n",
    "            v for k, v in PROMPTING_ID_DISPLAY_MAPPING.items() if \"agent\" not in k.lower()\n",
    "        ]\n",
    "\n",
    "    model_order = list(LLM_MODELS.values())\n",
    "    task_order = [\"Mortality\", \"AKI\", \"Sepsis\"]\n",
    "    dataset_order = [\"HiRID\", \"MIMIC-IV\", \"eICU\"]\n",
    "\n",
    "    # Prepare columns for output\n",
    "    metric_cols = [\n",
    "        \"AUROC\", \"AUPRC\", \"Normalized AUPRC\", \"Min(+P, Se)\", \"Sensitivity (Recall)\",\n",
    "        \"Specificity\", \"Precision\", \"F1 Score\", \"Accuracy\", \"Balanced Accuracy\", \"MCC\", \"Cohen's Kappa\"\n",
    "    ]\n",
    "\n",
    "    # Ensure all metric columns exist\n",
    "    for col in metric_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "\n",
    "    # Set correct column name for second column\n",
    "    if is_agent is None:\n",
    "        second_col = \"Prompting/Agent\"\n",
    "    else:\n",
    "        second_col = \"Agent\" if is_agent else \"Prompting Method\"\n",
    "\n",
    "    rows = []\n",
    "    for model in model_order:\n",
    "        for prompting in prompting_order:\n",
    "            for task in task_order:\n",
    "                for dataset in dataset_order:\n",
    "                    row = df[\n",
    "                        (df[\"Model\"] == model)\n",
    "                        & (df[\"prompting_id\"] == prompting)\n",
    "                        & (df[\"Task\"] == task)\n",
    "                        & (df[\"Dataset\"] == dataset)\n",
    "                    ]\n",
    "                    row_dict = {\n",
    "                        \"Model\": model,\n",
    "                        second_col: prompting,\n",
    "                        \"Task\": task,\n",
    "                        \"Dataset\": dataset,\n",
    "                    }\n",
    "                    if not row.empty:\n",
    "                        for col in metric_cols:\n",
    "                            row_dict[col] = row.iloc[0][col]\n",
    "                    else:\n",
    "                        for col in metric_cols:\n",
    "                            row_dict[col] = \"\"\n",
    "                    rows.append(row_dict)\n",
    "\n",
    "    df_out = pd.DataFrame(rows)\n",
    "\n",
    "    # Replace repeated values with empty string for hierarchical effect\n",
    "    for col in [\"Model\", second_col, \"Task\"]:\n",
    "        last_val = None\n",
    "        for i in range(len(df_out)):\n",
    "            if df_out.loc[i, col] == last_val:\n",
    "                df_out.loc[i, col] = \"\"\n",
    "            else:\n",
    "                last_val = df_out.loc[i, col]\n",
    "\n",
    "    df_out.to_csv(save_path, index=False)\n",
    "    print(f\"Saved LLM metrics table (is_agent={is_agent}) to {save_path}\")\n",
    "\n",
    "# Usage:\n",
    "# Save all LLM metrics (both agent and non-agent) into one file\n",
    "save_llm_metrics_table(\n",
    "    df_metrics,\n",
    "    is_agent=None,\n",
    "    save_path=\"./notebook_output/metrics_tables/llm_all_metrics.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2e9bc0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved paper-ready table for Mortality: ./notebook_output/metrics_tables/paper_auroc_auprc_mortality.csv\n",
      "Saved paper-ready table for AKI: ./notebook_output/metrics_tables/paper_auroc_auprc_aki.csv\n",
      "Saved paper-ready table for Sepsis: ./notebook_output/metrics_tables/paper_auroc_auprc_sepsis.csv\n"
     ]
    }
   ],
   "source": [
    "def save_paper_ready_metrics_tables(df_metrics, output_dir=\"./notebook_output/metrics_tables\"):\n",
    "    \"\"\"\n",
    "    Save one CSV per task (mortality, aki, sepsis) in the publication-ready format.\n",
    "    Only AUROC and AUPRC are included. Structure matches the provided screenshot.\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    # --- Define all groupings and orders ---\n",
    "    tasks = [\"Mortality\", \"AKI\", \"Sepsis\"]\n",
    "    datasets = [\"HiRID\", \"MIMIC-IV\", \"eICU\"]\n",
    "    metric_cols = [\"AUROC\", \"AUPRC\"]\n",
    "\n",
    "    # Baseline models\n",
    "    convml_models = [\"RandomForest\", \"XGBoost\", \"LightGBM\"]\n",
    "    convdl_models = [\"CNN\", \"InceptionTime\", \"LSTM\", \"GRU\"]\n",
    "\n",
    "    # LLMs\n",
    "    proprietary_llms = [\"OpenAI-o3\", \"Claude-Sonnet-4\", \"Grok-4\", \"Gemini-2.5-Pro\", \"Gemini-2.5-Flash\"]\n",
    "    open_llms = [\"Llama-3.1-8B-Instruct\", \"Deepseek-R1-Distill-Llama-8B\", \"Mistral-7B-Instruct-v0.3\", \"Gemma-3-4B-it\", \"MedGemma-4B-it\"]\n",
    "\n",
    "    # Prompting methods (order and display names)\n",
    "    prompting_methods = [\n",
    "        (\"Aggregation (Sarvari et al., 2024)\", \"Aggregation\"),\n",
    "        (\"Zero-Shot (Zhu et al., 2024b)\", \"Zero-Shot\"),\n",
    "        (\"One-Shot (Zhu et al., 2024b)\", \"One-Shot\"),\n",
    "        (\"Few-Shot (3) (Liu et al., 2023)\", \"Few-Shot (3)\"),\n",
    "        (\"CoT (Zhu et al., 2024a)\", \"CoT\"),\n",
    "    ]\n",
    "    agentic_methods = [\n",
    "        (\"SumAgent\", \"SumAgent\"),\n",
    "        (\"ColAgent\", \"ColAgent\"),\n",
    "        (\"ClinFlowAgent\", \"ClinFlowAgent\"),\n",
    "        (\"HybReAgent\", \"HybReAgent\"),\n",
    "    ]\n",
    "\n",
    "    # Helper for LLM model category\n",
    "    def get_llm_model_category(model):\n",
    "        return \"Proprietary LLM\" if model in proprietary_llms else \"Open-Source LLM\"\n",
    "\n",
    "    # Helper for Method Category\n",
    "    def get_method_category(method):\n",
    "        return \"Standard Prompting\" if method in [x[0] for x in prompting_methods] else \"Agentic\"\n",
    "\n",
    "    # --- Build and save table for each task ---\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for task in tasks:\n",
    "        rows = []\n",
    "        # Baseline rows\n",
    "        for method_cat, method, model_cat, model in [\n",
    "            (\"Baseline\", \"\", \"convML\", m) for m in convml_models\n",
    "        ] + [\n",
    "            (\"Baseline\", \"\", \"convDL\", m) for m in convdl_models\n",
    "        ]:\n",
    "            row = {\n",
    "                \"Method Cat\": method_cat,\n",
    "                \"Method\": method,\n",
    "                \"Model Category\": model_cat,\n",
    "                \"Model\": model,\n",
    "            }\n",
    "            for ds in datasets:\n",
    "                for metric in metric_cols:\n",
    "                    val = df_metrics.loc[\n",
    "                        (df_metrics[\"Model\"] == model)\n",
    "                        & (df_metrics[\"Task\"] == task)\n",
    "                        & (df_metrics[\"Dataset\"] == ds)\n",
    "                        & (df_metrics[\"subgroup_type\"] == \"Overall\"),\n",
    "                        metric,\n",
    "                    ]\n",
    "                    row[f\"{ds} {metric}\"] = f\"{val.iloc[0]:.3f}\" if not val.empty and pd.notnull(val.iloc[0]) else \"\"\n",
    "            rows.append(row)\n",
    "\n",
    "        # LLM rows (Standard Prompting)\n",
    "        for method_disp, method_key in prompting_methods:\n",
    "            for model_cat, model_list in [(\"Proprietary LLM\", proprietary_llms), (\"Open-Source LLM\", open_llms)]:\n",
    "                for model in model_list:\n",
    "                    row = {\n",
    "                        \"Method Cat\": \"Standard Prompting\",\n",
    "                        \"Method\": method_disp,\n",
    "                        \"Model Category\": model_cat,\n",
    "                        \"Model\": model,\n",
    "                    }\n",
    "                    for ds in datasets:\n",
    "                        for metric in metric_cols:\n",
    "                            val = df_metrics.loc[\n",
    "                                (df_metrics[\"Model\"] == model)\n",
    "                                & (df_metrics[\"Task\"] == task)\n",
    "                                & (df_metrics[\"Dataset\"] == ds)\n",
    "                                & (df_metrics[\"prompting_id\"] == method_key)\n",
    "                                & (df_metrics[\"subgroup_type\"] == \"Overall\"),\n",
    "                                metric,\n",
    "                            ]\n",
    "                            row[f\"{ds} {metric}\"] = f\"{val.iloc[0]:.3f}\" if not val.empty and pd.notnull(val.iloc[0]) else \"\"\n",
    "                    rows.append(row)\n",
    "\n",
    "        # LLM rows (Agentic)\n",
    "        for method_disp, method_key in agentic_methods:\n",
    "            for model_cat, model_list in [(\"Proprietary LLM\", proprietary_llms), (\"Open-Source LLM\", open_llms)]:\n",
    "                for model in model_list:\n",
    "                    row = {\n",
    "                        \"Method Cat\": \"Agentic\",\n",
    "                        \"Method\": method_disp,\n",
    "                        \"Model Category\": model_cat,\n",
    "                        \"Model\": model,\n",
    "                    }\n",
    "                    for ds in datasets:\n",
    "                        for metric in metric_cols:\n",
    "                            val = df_metrics.loc[\n",
    "                                (df_metrics[\"Model\"] == model)\n",
    "                                & (df_metrics[\"Task\"] == task)\n",
    "                                & (df_metrics[\"Dataset\"] == ds)\n",
    "                                & (df_metrics[\"prompting_id\"] == method_key)\n",
    "                                & (df_metrics[\"subgroup_type\"] == \"Overall\"),\n",
    "                                metric,\n",
    "                            ]\n",
    "                            row[f\"{ds} {metric}\"] = f\"{val.iloc[0]:.3f}\" if not val.empty and pd.notnull(val.iloc[0]) else \"\"\n",
    "                    rows.append(row)\n",
    "\n",
    "        # --- Build DataFrame and save ---\n",
    "        col_order = [\n",
    "            \"Method Cat\", \"Method\", \"Model Category\", \"Model\",\n",
    "            \"HiRID AUROC\", \"HiRID AUPRC\",\n",
    "            \"MIMIC-IV AUROC\", \"MIMIC-IV AUPRC\",\n",
    "            \"eICU AUROC\", \"eICU AUPRC\",\n",
    "        ]\n",
    "        df_out = pd.DataFrame(rows)\n",
    "        # Ensure all columns exist\n",
    "        for col in col_order:\n",
    "            if col not in df_out.columns:\n",
    "                df_out[col] = \"\"\n",
    "        df_out = df_out[col_order]\n",
    "\n",
    "        # Hierarchical effect: replace repeated values with \"\"\n",
    "        for col in [\"Method Cat\", \"Method\", \"Model Category\"]:\n",
    "            last_val = None\n",
    "            for i in range(len(df_out)):\n",
    "                if df_out.loc[i, col] == last_val:\n",
    "                    df_out.loc[i, col] = \"\"\n",
    "                else:\n",
    "                    last_val = df_out.loc[i, col]\n",
    "\n",
    "        save_path = os.path.join(output_dir, f\"paper_auroc_auprc_{task.lower()}.csv\")\n",
    "        df_out.to_csv(save_path, index=False)\n",
    "        print(f\"Saved paper-ready table for {task}: {save_path}\")\n",
    "\n",
    "# Usage:\n",
    "save_paper_ready_metrics_tables(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776a93e9",
   "metadata": {},
   "source": [
    "### Operational Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "76845d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all-level operational metrics to ./notebook_output/data/pulse_metrics_operational.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>model_name</th>\n",
       "      <th>is_agent</th>\n",
       "      <th>prompting_id</th>\n",
       "      <th>task</th>\n",
       "      <th>dataset</th>\n",
       "      <th>aggregation_level</th>\n",
       "      <th>sample_count</th>\n",
       "      <th>avg_num_steps</th>\n",
       "      <th>avg_step_tokenization_time</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_step_output_tokens</th>\n",
       "      <th>avg_sample_tokenization_time</th>\n",
       "      <th>avg_sample_inference_time</th>\n",
       "      <th>avg_sample_input_tokens</th>\n",
       "      <th>avg_sample_output_tokens</th>\n",
       "      <th>total_tokenization_time</th>\n",
       "      <th>total_inference_time</th>\n",
       "      <th>total_input_tokens</th>\n",
       "      <th>total_output_tokens</th>\n",
       "      <th>model_prompting_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LLM</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>model_type</td>\n",
       "      <td>383638</td>\n",
       "      <td>2.3441</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>...</td>\n",
       "      <td>308.7802</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>27.3315</td>\n",
       "      <td>6103.3365</td>\n",
       "      <td>763.5133</td>\n",
       "      <td>1859.4811</td>\n",
       "      <td>1.005299e+07</td>\n",
       "      <td>1819684243</td>\n",
       "      <td>286565447</td>\n",
       "      <td>all, all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LLM</td>\n",
       "      <td>Claude-Sonnet-4</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>model_type+model_name</td>\n",
       "      <td>12378</td>\n",
       "      <td>2.1314</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>869.9855</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>35.4292</td>\n",
       "      <td>4174.4118</td>\n",
       "      <td>1672.5284</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.372326e+05</td>\n",
       "      <td>43922761</td>\n",
       "      <td>20892503</td>\n",
       "      <td>Claude-Sonnet-4, all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LLM</td>\n",
       "      <td>Deepseek-R1-Distill-Llama-8B</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>model_type+model_name</td>\n",
       "      <td>55701</td>\n",
       "      <td>2.4917</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>...</td>\n",
       "      <td>930.4386</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>68.3631</td>\n",
       "      <td>5808.6491</td>\n",
       "      <td>2264.5434</td>\n",
       "      <td>300.5096</td>\n",
       "      <td>3.580474e+06</td>\n",
       "      <td>260729707</td>\n",
       "      <td>123696223</td>\n",
       "      <td>Deepseek-R1-Distill-Llama-8B, all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LLM</td>\n",
       "      <td>Gemini-2.5-Flash</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>model_type+model_name</td>\n",
       "      <td>55701</td>\n",
       "      <td>2.3231</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>174.7053</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.0338</td>\n",
       "      <td>6563.2477</td>\n",
       "      <td>430.2607</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.684484e+05</td>\n",
       "      <td>275650601</td>\n",
       "      <td>23091933</td>\n",
       "      <td>Gemini-2.5-Flash, all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LLM</td>\n",
       "      <td>Gemini-2.5-Pro</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>model_type+model_name</td>\n",
       "      <td>12378</td>\n",
       "      <td>2.1811</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>139.9383</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>44.4366</td>\n",
       "      <td>4510.8939</td>\n",
       "      <td>322.6688</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.522096e+05</td>\n",
       "      <td>45122663</td>\n",
       "      <td>4144375</td>\n",
       "      <td>Gemini-2.5-Pro, all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_type                    model_name is_agent prompting_id task dataset  \\\n",
       "0        LLM                           all      all          all  all     all   \n",
       "1        LLM               Claude-Sonnet-4      all          all  all     all   \n",
       "2        LLM  Deepseek-R1-Distill-Llama-8B      all          all  all     all   \n",
       "3        LLM              Gemini-2.5-Flash      all          all  all     all   \n",
       "4        LLM                Gemini-2.5-Pro      all          all  all     all   \n",
       "\n",
       "       aggregation_level  sample_count  avg_num_steps  \\\n",
       "0             model_type        383638         2.3441   \n",
       "1  model_type+model_name         12378         2.1314   \n",
       "2  model_type+model_name         55701         2.4917   \n",
       "3  model_type+model_name         55701         2.3231   \n",
       "4  model_type+model_name         12378         2.1811   \n",
       "\n",
       "   avg_step_tokenization_time  ...  avg_step_output_tokens  \\\n",
       "0                      0.0041  ...                308.7802   \n",
       "1                      0.0000  ...                869.9855   \n",
       "2                      0.0067  ...                930.4386   \n",
       "3                      0.0000  ...                174.7053   \n",
       "4                      0.0000  ...                139.9383   \n",
       "\n",
       "   avg_sample_tokenization_time  avg_sample_inference_time  \\\n",
       "0                        0.0059                    27.3315   \n",
       "1                        0.0000                    35.4292   \n",
       "2                        0.0070                    68.3631   \n",
       "3                        0.0000                     5.0338   \n",
       "4                        0.0000                    44.4366   \n",
       "\n",
       "   avg_sample_input_tokens  avg_sample_output_tokens  total_tokenization_time  \\\n",
       "0                6103.3365                  763.5133                1859.4811   \n",
       "1                4174.4118                 1672.5284                   0.0000   \n",
       "2                5808.6491                 2264.5434                 300.5096   \n",
       "3                6563.2477                  430.2607                   0.0000   \n",
       "4                4510.8939                  322.6688                   0.0000   \n",
       "\n",
       "   total_inference_time  total_input_tokens  total_output_tokens  \\\n",
       "0          1.005299e+07          1819684243            286565447   \n",
       "1          4.372326e+05            43922761             20892503   \n",
       "2          3.580474e+06           260729707            123696223   \n",
       "3          2.684484e+05           275650601             23091933   \n",
       "4          5.522096e+05            45122663              4144375   \n",
       "\n",
       "                  model_prompting_id  \n",
       "0                           all, all  \n",
       "1               Claude-Sonnet-4, all  \n",
       "2  Deepseek-R1-Distill-Llama-8B, all  \n",
       "3              Gemini-2.5-Flash, all  \n",
       "4                Gemini-2.5-Pro, all  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calculate_llm_operational_metrics(df_metadata):\n",
    "    \"\"\"\n",
    "    Calculate average and total operational metrics for LLM models at all aggregation levels.\n",
    "    For higher aggregation levels, average the metrics from the most fine-grained level.\n",
    "    Ensures consistency between avg step and avg sample metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    llm_mask = df_metadata[\"model_type\"] == \"LLM\"\n",
    "    df_metadata_llm = df_metadata[llm_mask].copy()\n",
    "    if df_metadata_llm.empty:\n",
    "        print(\"No LLM models found in metadata. Skipping operational metrics.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Remove unused categories\n",
    "    for col in [\n",
    "        \"model_type\",\n",
    "        \"model_name\",\n",
    "        \"prompting_id\",\n",
    "        \"task\",\n",
    "        \"dataset\",\n",
    "        \"is_agent\",\n",
    "    ]:\n",
    "        if col in df_metadata_llm.columns and isinstance(\n",
    "            df_metadata_llm[col].dtype, pd.CategoricalDtype\n",
    "        ):\n",
    "            df_metadata_llm[col] = df_metadata_llm[col].cat.remove_unused_categories()\n",
    "\n",
    "    # Ensure columns exist\n",
    "    for col in [\n",
    "        \"Tokenization Time\",\n",
    "        \"Inference Time\",\n",
    "        \"Input Tokens\",\n",
    "        \"Output Tokens\",\n",
    "        \"sample_index\",\n",
    "        \"Step Name\",\n",
    "        \"Step Number\",\n",
    "    ]:\n",
    "        if col not in df_metadata_llm.columns:\n",
    "            df_metadata_llm[col] = np.nan\n",
    "\n",
    "    all_cols = [\n",
    "        \"model_type\",\n",
    "        \"model_name\",\n",
    "        \"is_agent\",\n",
    "        \"prompting_id\",\n",
    "        \"task\",\n",
    "        \"dataset\",\n",
    "    ]\n",
    "    fine_grained_level = [\n",
    "        \"model_type\",\n",
    "        \"model_name\",\n",
    "        \"is_agent\",\n",
    "        \"prompting_id\",\n",
    "        \"task\",\n",
    "        \"dataset\",\n",
    "    ]\n",
    "    agg_levels = [\n",
    "        ([\"model_type\"], \"model_type\"),\n",
    "        ([\"model_type\", \"model_name\"], \"model_type+model_name\"),\n",
    "        ([\"model_type\", \"model_name\", \"is_agent\"], \"model_type+model_name+is_agent\"),\n",
    "        (\n",
    "            [\"model_type\", \"model_name\", \"is_agent\", \"prompting_id\"],\n",
    "            \"model_type+model_name+is_agent+prompting_id\",\n",
    "        ),\n",
    "        (\n",
    "            [\"model_type\", \"model_name\", \"is_agent\", \"prompting_id\", \"task\"],\n",
    "            \"model_type+model_name+is_agent+prompting_id+task\",\n",
    "        ),\n",
    "        (\n",
    "            [\"model_type\", \"model_name\", \"is_agent\", \"prompting_id\", \"task\", \"dataset\"],\n",
    "            \"model_type+model_name+is_agent+prompting_id+task+dataset\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # --- Step 1: Calculate fine-grained metrics ---\n",
    "    fine_metrics = []\n",
    "    groupby_obj = df_metadata_llm.groupby(fine_grained_level, observed=True)\n",
    "    for keys, group in groupby_obj:\n",
    "        is_agent = group[\"is_agent\"].iloc[0] if \"is_agent\" in group else False\n",
    "        if is_agent:\n",
    "            group_filtered = group[~(group[\"Step Name\"] == \"SAMPLE_METADATA\")]\n",
    "        else:\n",
    "            group_filtered = group\n",
    "\n",
    "        sample_count = len(group_filtered)\n",
    "        if \"sample_index\" in group_filtered.columns and len(group_filtered) > 0:\n",
    "            n_unique_sample_index = group_filtered[\"sample_index\"].nunique()\n",
    "            avg_num_steps = (\n",
    "                sample_count / n_unique_sample_index\n",
    "                if n_unique_sample_index > 0\n",
    "                else np.nan\n",
    "            )\n",
    "\n",
    "            sample_groups = group_filtered.groupby(\"sample_index\", observed=True)\n",
    "            sample_tokenization = sample_groups[\"Tokenization Time\"].sum()\n",
    "            sample_inference = sample_groups[\"Inference Time\"].sum()\n",
    "            sample_input_tokens = sample_groups[\"Input Tokens\"].sum()\n",
    "            sample_output_tokens = sample_groups[\"Output Tokens\"].sum()\n",
    "\n",
    "            avg_sample_tokenization_time = (\n",
    "                sample_tokenization.mean() if len(sample_tokenization) > 0 else np.nan\n",
    "            )\n",
    "            avg_sample_inference_time = (\n",
    "                sample_inference.mean() if len(sample_inference) > 0 else np.nan\n",
    "            )\n",
    "            avg_sample_input_tokens = (\n",
    "                sample_input_tokens.mean() if len(sample_input_tokens) > 0 else np.nan\n",
    "            )\n",
    "            avg_sample_output_tokens = (\n",
    "                sample_output_tokens.mean() if len(sample_output_tokens) > 0 else np.nan\n",
    "            )\n",
    "        else:\n",
    "            avg_num_steps = np.nan if is_agent else 1\n",
    "            avg_sample_tokenization_time = (\n",
    "                group_filtered[\"Tokenization Time\"].mean()\n",
    "                if len(group_filtered) > 0\n",
    "                else np.nan\n",
    "            )\n",
    "            avg_sample_inference_time = (\n",
    "                group_filtered[\"Inference Time\"].mean()\n",
    "                if len(group_filtered) > 0\n",
    "                else np.nan\n",
    "            )\n",
    "            avg_sample_input_tokens = (\n",
    "                group_filtered[\"Input Tokens\"].mean()\n",
    "                if len(group_filtered) > 0\n",
    "                else np.nan\n",
    "            )\n",
    "            avg_sample_output_tokens = (\n",
    "                group_filtered[\"Output Tokens\"].mean()\n",
    "                if len(group_filtered) > 0\n",
    "                else np.nan\n",
    "            )\n",
    "\n",
    "        # Adjust sample_count for is_agent==True\n",
    "        adj_sample_count = sample_count\n",
    "        if is_agent and avg_num_steps > 0:\n",
    "            adj_sample_count = sample_count / avg_num_steps\n",
    "            if not np.isclose(adj_sample_count, round(adj_sample_count)):\n",
    "                print(\n",
    "                    f\"[WARNING] Adjusted sample_count is not integer for group {keys}: {adj_sample_count}\"\n",
    "                )\n",
    "            adj_sample_count = int(round(adj_sample_count))\n",
    "\n",
    "        # Step metrics (per row)\n",
    "        avg_step_tokenization_time = (\n",
    "            group_filtered[\"Tokenization Time\"].mean()\n",
    "            if len(group_filtered) > 0\n",
    "            else np.nan\n",
    "        )\n",
    "        avg_step_inference_time = (\n",
    "            group_filtered[\"Inference Time\"].mean()\n",
    "            if len(group_filtered) > 0\n",
    "            else np.nan\n",
    "        )\n",
    "        avg_step_input_tokens = (\n",
    "            group_filtered[\"Input Tokens\"].mean() if len(group_filtered) > 0 else np.nan\n",
    "        )\n",
    "        avg_step_output_tokens = (\n",
    "            group_filtered[\"Output Tokens\"].mean()\n",
    "            if len(group_filtered) > 0\n",
    "            else np.nan\n",
    "        )\n",
    "\n",
    "        # Total metrics\n",
    "        total_tokenization_time = group_filtered[\"Tokenization Time\"].sum()\n",
    "        total_inference_time = group_filtered[\"Inference Time\"].sum()\n",
    "        total_input_tokens = group_filtered[\"Input Tokens\"].sum()\n",
    "        total_output_tokens = group_filtered[\"Output Tokens\"].sum()\n",
    "\n",
    "        fine_metrics.append(\n",
    "            {\n",
    "                **dict(\n",
    "                    zip(fine_grained_level, keys if isinstance(keys, tuple) else [keys])\n",
    "                ),\n",
    "                \"sample_count\": adj_sample_count,\n",
    "                \"avg_num_steps\": avg_num_steps,\n",
    "                \"avg_step_tokenization_time\": avg_step_tokenization_time,\n",
    "                \"avg_step_inference_time\": avg_step_inference_time,\n",
    "                \"avg_step_input_tokens\": avg_step_input_tokens,\n",
    "                \"avg_step_output_tokens\": avg_step_output_tokens,\n",
    "                \"avg_sample_tokenization_time\": avg_sample_tokenization_time,\n",
    "                \"avg_sample_inference_time\": avg_sample_inference_time,\n",
    "                \"avg_sample_input_tokens\": avg_sample_input_tokens,\n",
    "                \"avg_sample_output_tokens\": avg_sample_output_tokens,\n",
    "                \"total_tokenization_time\": total_tokenization_time,\n",
    "                \"total_inference_time\": total_inference_time,\n",
    "                \"total_input_tokens\": total_input_tokens,\n",
    "                \"total_output_tokens\": total_output_tokens,\n",
    "            }\n",
    "        )\n",
    "    fine_df = pd.DataFrame(fine_metrics)\n",
    "\n",
    "    # --- Step 2: Aggregate for higher levels ---\n",
    "    all_results = []\n",
    "    for level, level_name in agg_levels:\n",
    "        # For each group at this level, average the fine-grained metrics\n",
    "        grouped = fine_df.groupby(level, observed=True)\n",
    "        agg_rows = []\n",
    "        for keys, group in grouped:\n",
    "            # For sample_count, sum (not mean)\n",
    "            sample_count = group[\"sample_count\"].sum()\n",
    "            # For all other metrics, take mean\n",
    "            avg_num_steps = group[\"avg_num_steps\"].mean()\n",
    "            avg_step_tokenization_time = group[\"avg_step_tokenization_time\"].mean()\n",
    "            avg_step_inference_time = group[\"avg_step_inference_time\"].mean()\n",
    "            avg_step_input_tokens = group[\"avg_step_input_tokens\"].mean()\n",
    "            avg_step_output_tokens = group[\"avg_step_output_tokens\"].mean()\n",
    "            avg_sample_tokenization_time = group[\"avg_sample_tokenization_time\"].mean()\n",
    "            avg_sample_inference_time = group[\"avg_sample_inference_time\"].mean()\n",
    "            avg_sample_input_tokens = group[\"avg_sample_input_tokens\"].mean()\n",
    "            avg_sample_output_tokens = group[\"avg_sample_output_tokens\"].mean()\n",
    "            total_tokenization_time = group[\"total_tokenization_time\"].sum()\n",
    "            total_inference_time = group[\"total_inference_time\"].sum()\n",
    "            total_input_tokens = group[\"total_input_tokens\"].sum()\n",
    "            total_output_tokens = group[\"total_output_tokens\"].sum()\n",
    "\n",
    "            row = {\n",
    "                **dict(zip(level, keys if isinstance(keys, tuple) else [keys])),\n",
    "                \"aggregation_level\": level_name,\n",
    "                \"sample_count\": sample_count,\n",
    "                \"avg_num_steps\": avg_num_steps,\n",
    "                \"avg_step_tokenization_time\": avg_step_tokenization_time,\n",
    "                \"avg_step_inference_time\": avg_step_inference_time,\n",
    "                \"avg_step_input_tokens\": avg_step_input_tokens,\n",
    "                \"avg_step_output_tokens\": avg_step_output_tokens,\n",
    "                \"avg_sample_tokenization_time\": avg_sample_tokenization_time,\n",
    "                \"avg_sample_inference_time\": avg_sample_inference_time,\n",
    "                \"avg_sample_input_tokens\": avg_sample_input_tokens,\n",
    "                \"avg_sample_output_tokens\": avg_sample_output_tokens,\n",
    "                \"total_tokenization_time\": total_tokenization_time,\n",
    "                \"total_inference_time\": total_inference_time,\n",
    "                \"total_input_tokens\": total_input_tokens,\n",
    "                \"total_output_tokens\": total_output_tokens,\n",
    "            }\n",
    "            # Fill missing columns with 'all'\n",
    "            for col in all_cols:\n",
    "                if col not in row:\n",
    "                    row[col] = \"all\"\n",
    "            agg_rows.append(row)\n",
    "        agg_df = pd.DataFrame(agg_rows)\n",
    "        # Reorder columns\n",
    "        agg_df = agg_df[\n",
    "            all_cols\n",
    "            + [\n",
    "                \"aggregation_level\",\n",
    "                \"sample_count\",\n",
    "                \"avg_num_steps\",\n",
    "                \"avg_step_tokenization_time\",\n",
    "                \"avg_step_inference_time\",\n",
    "                \"avg_step_input_tokens\",\n",
    "                \"avg_step_output_tokens\",\n",
    "                \"avg_sample_tokenization_time\",\n",
    "                \"avg_sample_inference_time\",\n",
    "                \"avg_sample_input_tokens\",\n",
    "                \"avg_sample_output_tokens\",\n",
    "                \"total_tokenization_time\",\n",
    "                \"total_inference_time\",\n",
    "                \"total_input_tokens\",\n",
    "                \"total_output_tokens\",\n",
    "            ]\n",
    "        ]\n",
    "        all_results.append(agg_df)\n",
    "\n",
    "    combined = pd.concat(all_results, ignore_index=True)\n",
    "    float_cols = combined.select_dtypes(include=[\"float\"]).columns\n",
    "    combined[float_cols] = combined[float_cols].round(4)\n",
    "    for col in [\"total_input_tokens\", \"total_output_tokens\"]:\n",
    "        if col in combined.columns:\n",
    "            combined[col] = combined[col].fillna(0).astype(int)\n",
    "    return combined\n",
    "\n",
    "\n",
    "# Calculate operational metrics\n",
    "df_metrics_operational = calculate_llm_operational_metrics(df_metadata)\n",
    "\n",
    "# Add model_prompting_id column to operational metrics DataFrame\n",
    "if 'model_name' in df_metrics_operational.columns and 'model_type' in df_metrics_operational.columns:\n",
    "    def get_model_prompting_id_operational(row):\n",
    "        if row.get('model_type') == 'LLM' and row.get('prompting_id', ''):\n",
    "            return f\"{row['model_name']}, {row['prompting_id']}\"\n",
    "        else:\n",
    "            return row['model_name']\n",
    "    df_metrics_operational['model_prompting_id'] = df_metrics_operational.apply(get_model_prompting_id_operational, axis=1)\n",
    "\n",
    "# Save df_metrics_operational\n",
    "os.makedirs(\"./notebook_output/postprocessed_data\", exist_ok=True)\n",
    "save_path_operational = \"./notebook_output/postprocessed_data/pulse_metrics_operational.csv\"\n",
    "df_metrics_operational.to_csv(save_path_operational, index=False)\n",
    "print(f\"Saved all-level operational metrics to {save_path_operational}\")\n",
    "display(df_metrics_operational.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pulse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
